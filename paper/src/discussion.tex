\section{Discussion}
\label{sec:discussion}

We have made two main contributions with this paper:
(1) described the latent subjectivity problem and
(2) developed the technique of adaptive recommenders.

(1) The latent subjectivity problem is one we think hinders
standard recommender systems reaching their full potential.
As far as we know, this problem has not been described
in the context of recommender systems.
The main choice for any such system is how to predict unknown ratings.
To do this, a pattern in the available ratings data must be leveraged.
These patterns are plentiful, and their individual performance
depends on each user and item of the system.
Modern aggregation recommenders utilize many patterns, but on a generalized
level, where each user and item is treated the same.
This underlying subjectivity leads to a mismatch between the notions
of whoever developed the systems, and the users and items of the service.

Averaged or generalized weighted approaches will always
chose the combination that performs best \emph{on average},
with little concern to the uniqueness of items (and users).
In other words, this is a comprehensive problem
that may be discovered amongst many machine learning techniques.

(2) Adaptive recommenders is our attempt to solve the latent subjectivity problem.
As far as we know, this type of adaptive prediction aggregation has not been done before.
Section \ref{sec:results} showed that an aggregation that combines predictions based
on estimated accuracy can outperform both standard recommenders and simple aggregation approaches.
Our technique is strengthened by the fact that standard recommender algorithms
are used for the accuracy estimations.
This is the core insight of this paper.
\emph{By creating error models for each recommender, we can use this to predict
its accuracy for each user/item combination.
These predictions can then be used to weigh each combined algorithm accordingly.}

We believe there are greater opportunities in systems where there  are even more diverging
patterns to be leveraged. The prime examples of this are systems that may or may 
not use social connections between users, and systems that predict the 
relevance of widely varying items.

\subsection{Limitations}

There are some important general limitations to our research
related to 
(1) the complexity of our method, 
(2) our choice of data and evaluation metrics, 
(3) the general usefulness of this approach, and
(4) common issues with recommender systems.

\emph{(1) Complexity:} As our approach is more complicated than standard recommenders,
it is worth questioning whether or not the performance gains are worth the added complexity.
This depends on the basic recommenders that are to be combined.
If the system is made up by many different recommenders
that each user might place varying importance on,
and that may have varying success with each item,
adaptive recommenders may provide gains in accuracy.

On the other hand, if the recommenders are simple in nature,
and look at similar patterns in the data,
generalized aggregation methods might be more applicable.
Clearly, the performance gains in our experiments
are not substantial enough to declare anything without reservation.
While we believe this technique has potential,
without real-world success stories, it is hard
to suggest that our method is particularly better
than a simple standard recommender.

\emph{(2) Evaluation:} we chose traditional datasets and evaluation metrics
to validate the adaptive recommenders technique.
While our initial results are promising, it is important to stress
that this is only one test on one dataset. Considering the vast scope
of applicable data, and the number of ways these results many be 
evaluated, the results must be seen for what they are,
that is, initial and preliminary explorations of a new technique
that has yet to be proved useful in the real world.

\emph{(3) Usefulness:}
When considering the additional complexity of our approach,
a natural response is whether or not current approaches
to recommender systems are good enough.
We do not think so. Information overload is such a nuanced problem 
that the only solution lies in intelligent, adaptive systems.
However, as most current recommender systems 
perform quite simple tasks, they may be more
than good enough for their purpose.

There will always be a trade-off, between complexity and required accuracy.
As in many other cases, each of the systems described in this paper
have their use cases. In the end, the requirements of each system
must decide which method best suit their needs.

\emph{(4) Common issues:}
The topic of recommenders and adaptive systems in general
raise a number of questions which is outside the scope of this paper.
For example, user privacy is a big issue.
Whenever we have a system that tries to learn the tastes, habits and
traits of its users, how each user will react to this must be considered.
This is often a trade-off between adaptability and transparency.
The most adaptive systems will not always be able to explain to the users
what is going on and what it knows about each person,
especially when dealing with emergent behavior based on 
numerical user models.

In addition to the general limitation, our experiment carries a few drawbacks.
Our method was only tested against a limited number of standard recommenders.
The key word is standard: these recommenders were not heavily customized
to fit the available data. As in all machine learning,
achieving relatively good performance is quite simple.
Any improvements above this standard requires deep domain knowledge,
and methods customized to the problem at hand.
In an actual system, the adaptive recommenders should be tested
against carefully selected standard recommenders,
optimized for the current domain.

Similarly, our method was only tested against simple aggregators.
Many more complex aggregations are possible,
for example by solving the problem of finding
optimal generalized weights for each method.


