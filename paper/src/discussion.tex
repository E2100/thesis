\section{Discussion}
\label{sec:discussion}

We have made two main contributions with this paper:
(1) we have described the latent subjectivity problem and
(2) we have developed the technique of adaptive recommenders.

\textbf{The latent subjectivity problem} is one we think hinders
standard recommender systems reaching their full potential.
As far as we know, this problem has not been described
in the context of recommender systems.
The main choice for any such system is how to predict unknown ratings.
To do this, a pattern in the available ratings data must be leveraged.
These patterns are plentiful, and their individual performance
depends on the users and items of the system.
Modern aggregation recommenders utilize many patterns, but on a generalized
level, where every user and item is treated the same.
This underlying subjectivity leads to a mismatch between the notions
of whoever developed the systems, and the users and items of the service.

Averaged or generalized weighted approaches will always
choose the combination that performs best \emph{on average},
with little concern to the uniqueness of items (and users).
In other words, this is a comprehensive problem
that may be discovered amongst many machine learning techniques.

\textbf{Adaptive recommenders} is our attempt to solve the latent subjectivity problem.
As far as we know, this type of adaptive prediction aggregation has not been done before.
Section \ref{sec:results} showed that an aggregation that combines predictions based
on estimated accuracy can outperform both standard recommenders and simple aggregation approaches.
Our technique is strengthened by the fact that standard recommender algorithms
are used for the accuracy estimations.
This is the core insight of this paper.
\emph{By creating error models for the recommenders, we can predict
their accuracy for each user/item combination.
These predictions can then be used to weigh the combined algorithms accordingly.}

We believe there are greater opportunities in systems where there  are even more diverging
patterns to be leveraged. The prime examples of this are systems that may or may 
not use social connections between users, and systems that predict the 
relevance of widely varying items.

\subsection{Limitations}

There are some important general limitations to our research
related to 
(1) the complexity of our method, 
(2) our choice of data and evaluation metrics, 
(3) the general usefulness of this approach, and
(4) common issues with recommender systems.

\emph{(1) Complexity:} As our approach is more complicated than standard recommenders,
it is worth questioning whether or not the performance gains are worth the added complexity.
This depends on the basic recommenders that are to be combined.
If the system is made up by many different recommenders
that each user might place varying importance on,
and that may have varying success with each item,
adaptive recommenders may provide gains in accuracy.

On the other hand, if the recommenders are simple in nature,
and look at similar patterns in the data,
generalized aggregation methods might be more applicable.
Clearly, the performance gains in our experiments
are not substantial enough to declare anything without reservation.

The computational cost of our proposed system would be about twice the cost of the systems we compare ourself to.
Modern recommenders already employ multiple methods, and our system would add another layer
of recommenders, doubling the number of recommender algorithms and amount of required computation.
However, the scalability of using multiple recommenders is not a new problem, and solutions do exist.
For instance, many of the methods, including our SVD-based aggregation recommenders allow for offline 
computation of the matrix factors, allowing for scalable predictions.

While we believe this technique has potential,
without real-world success stories, it is hard
to suggest that our method is particularly better
than a simple standard recommender.

\emph{(2) Evaluation:} we chose traditional datasets and evaluation metrics
to validate the adaptive recommenders technique.
While our initial results are promising, it is important to stress
that this is only one test on one dataset. Considering the vast scope
of applicable data, and the number of ways these results many be 
evaluated, the results must be seen for what they are,
that is, initial and preliminary explorations of a new technique
that has yet to be proved useful in the real world.

More specifically, we would like to perform the same experiments on the NetFlix dataset,
to be able to compare our performance to \cite{Bell2007}
(and other modern approaches that utilize this dataset).
However, this dataset is no longer available from NetFlix and 
redistribution seems to be prohibited.

\emph{(3) Usefulness:}
When considering the additional complexity of our approach,
a natural response is whether or not current approaches
to recommender systems are good enough.
We do not think so. Information overload is such a nuanced problem 
that the only solution lies in intelligent, adaptive systems.
However, as most current recommender systems 
perform quite simple tasks, they may be more
than good enough for their purpose.

There will always be a trade-off, between complexity and required accuracy.
As in many other cases, the systems described in this paper
have their use cases. In the end, the requirements of the system in question
must decide which method best suit their needs.

\emph{(4) Common issues:}
The topic of recommenders and adaptive systems in general
raise a number of questions which is outside the scope of this paper.
For example, user privacy is a big issue.
Whenever we have a system that tries to learn the tastes, habits and
traits of its users, how each user will react to this must be considered.
This is often a trade-off between adaptability and transparency.
The most adaptive systems will not always be able to explain to the users
what is going on and what it knows about each person,
especially when dealing with emergent behavior based on 
numerical user models.

In addition to the general limitation, our experiment carries a few drawbacks.
Our method was only tested against a limited number of standard recommenders.
The key word is standard: these recommenders were not heavily customized
to fit the available data. As in all machine learning,
achieving relatively good performance is quite simple.
Any improvements above this standard requires deep domain knowledge,
and methods customized to the problem at hand.
In an actual system, the adaptive recommenders should be tested
against carefully selected standard recommenders,
optimized for the current domain.
Other future work includes looking at how the error matrix
is influenced by common recommender system problems,
such as the cold start and average user problems.


Similarly, our method was only tested against simple aggregators.
Many more complex aggregations are possible,
for example by solving the problem of finding
optimal generalized weights for each method.


