\section{Introduction}
\label{sec:intro}

Information is a curious thing.
Having too much can be as harmful as having no information at all.
While lacking information is an obvious problem,
too much leads to information overload,
where relevant content drowns in irrelevant noise.
This is a common problem: whenever we have enough information,
any extra data only leads to confusion.
Our ability to make informed decisions is often the first thing to go
\cite[p1]{Davenport2001}.

However, while people struggle with excessive information,
many algorithms in \emph{artificial intelligence}  
can increase their performance by accessing more information.
\cite[p1]{Halevy2009} calls this the ``unreasonable effectiveness of data'':
perhaps surprisingly, more data often trumps more efficient algorithms.
For example, \cite[p3]{Banko2001} show how common algorithms in AI 
can be substantially improved by giving them a lot more data to work with.
As much as researchers chase elegant algorithms, finding more data to work with may be time better spent.

Few places is this difference of users and computers more apparent than in \emph{recommender systems}.
A recommender system is a technique in user modeling to estimate the relevance of an item to a user.
An item can be just about anything, for example documents, websites, movies, events or other users.
These systems use data such as search query logs, 
ratings from similar users, social connections and much more
to predict unknown relevance.
Recommender systems are especially prolific on the Web. 
Wherever there is personalized recommendations of news, books, movies,
articles, social connections, search results, et cetera, recommender systems are working behind the scenes.

Modern recommendation approaches often embrace the 
unreasonable effectiveness of data,
by combining multiple recommender systems, that each predict relevance in various ways.
By considering different aspects of users and items when making predictions,
the methods provide quite complex predictions that rely on much evidence.
For example, Bell et al. took this approach its logical conclusion in \cite[p1]{Bell2007}, by 
combining 107 different recommender systems when winning the 
Netflix movie recommender challenge
(see \cite{Linden2009}).

While the name ``recommender systems'' might seem limiting, these are incredibly powerful tools.
If we can accurately predict how each user will react to each item,
we will have come a long way towards solving information overload.

However, despite their apparent power, recommender systems are often confined
to simple tasks like creating small lists of recommended items
or suggesting similar items to the ones being considered by a user.
Common examples are lists of recommended items based on the one being viewed, 
recommending new social connections, or suggesting news articles based on previous reading.
Seldom are their full potential reached by creating completely adaptive
content systems, that work hard to mitigate any signs of information overload.

We posit that traditional recommender systems have an important weakness.
There exists an underlying, misplaced subjectivity to relevance prediction.
We believe this fundamental weakness hinders the full adoption of these systems.
There is a mismatch between how recommender systems perform predictions,
and how each user and item wants these predictions to be made.

Consider this: 
when an algorithm is selected for use in a recommender system,
there is a concious decision of which predictive data pattern to use.
Before any user modeling is performed, the researcher or developer selects
one or more methods that is thought to best model every user and item in the system.
While the methods themselves may perform well, their selection
reflects how whoever created the system assumes how each user
can and \emph{should} be modeled. This underlying subjectivity is not desirable.
We call this the \emph{latent subjectivity problem}
(see \cite[p33]{Bjorkoy2011}).

Examples are not hard to come by.
For instance, while one user might appreciate social
influence in their search results, another user might not.
While one user might find frequency of communication maps well to relevance,
another might not. 
One user might feel the similarity of movie titles are a good predictor,
while another might be more influenced by production year.
Some users may favor items rated highly on a global scale,
while others are more interested in what users similar to themselves have to say.
The same problem exists for items: while one item might best be judged by its content,
another might be better described by previous ratings from other users.
One item's relevance may be closely tied to when it was created,
while other items may be timeless.
The exact differences are not important, only that they exist.

%User modeling methods are dependent on the subjective assumptions of their creators.
%In other words, a modeling method use certian aspects of available data to make predictions,
%and these aspects are chosen by whoever creates the system.

Aggregate modeling methods face the same problem of misplaced subjectivity. 
Aggregation is done on a generalized, global level,
where each user and item is expected to place the same importance on each modeling method
(e.g. \cite{Aslam2001, Claypool1999, Bell2007, Carmel2009, Bender2005}).
While aggregation is selected to minimize some error over a testing set,
the subjective nature remains. The compiled aggregation is a generalization,
treating all users the same --- hardly a goal of user modeling.

%Should it not be up to each user to implicitly decide which method best describes their preferences?
%And, considering the vast scope of items we can come by, will the selected
%methods really perform optimally for every item?
We believe the priority of each algorithm should be implicitly and automatically
based on how well they have previously worked for each user and item.
%Without this adaptability, it may be hard for recommender systems
%to gain traction in scenarios with widely differing users and items.
The scope of users and items is simply too great for any one or generalized combination
of methods to capture the nuanced nature of relevance prediction.

We propose a novel method that we call \emph{adaptive recommenders}, where 
the selection of algorithms is made by each user and item.
This provides an extra level of abstraction and personalization.
The decisions are implicit, and happens in the background, without any extra interaction required.
%This leaves the subjective nature of selecting ways to model users and items where it should be, i.e.
%in the hands of each individual user, and dependent on each specific item.
This adaptive selection has an important consequence. 
If each algorithm is contextually used based on how well it performs for each user and item,
any possibly useful recommender algorithm suddenly becomes a worthy addition.

As far as we know, such adaptive prediction aggregation has not been done before.
In this paper, we explore the question of whether or not
adaptive recommenders can outperform traditional approaches.
