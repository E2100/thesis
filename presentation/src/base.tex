\documentclass[screen]{beamer}
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}

% Graphics
\RequirePackage{graphicx}
\RequirePackage{pdfpages}
\RequirePackage{wrapfig}
\RequirePackage{subfig}
\RequirePackage{caption}
\RequirePackage{tikz}
\RequirePackage{pgf}
\RequirePackage{pgfplots}
\usetikzlibrary{arrows,shapes,positioning,plotmarks}

% Color
\RequirePackage{xcolor}
\definecolor{red}{HTML}{990000}
\definecolor{green}{HTML}{336633}
\definecolor{black}{HTML}{000000}
\definecolor{lightgray}{HTML}{CCCCCC}
\definecolor{gray}{HTML}{999999}
\definecolor{darkgray}{HTML}{666666}

\usepackage{setspace}

% Bruk NTNU-temaet for beamer (her i bokmålvariant), alternativer er
% ntnunynorsk og ntnuenglish.
\usetheme{ntnuenglish}

\setbeamertemplate{footline}[ntnu theme nologo]
 
% Angi tittelen, vi gir også en kortere variant som brukes nederst på
% hver slide:
\title[Adaptive Aggregation of Recommender Systems]%
{Adaptive Aggregation of\\Recommender Systems}

% Angir foredragsholder, også en (valgfri) kortversjon i
% hakeparanteser først som kommer nederst på hver slide:
\author{Olav Bj{\o}rk{\o}y}

% Institusjon. Bruk gjerne disse slik det passer best med det du vil
% ha.  Valgfri kortversjon her også
\institute[NTNU]{Department of Computer and Information Science}

% Datoen blir også trykket på forsida. 
\date{October 3rd, 2011}
%\date{} % Bruk denne hvis du ikke vil ha noe dato på forsida.

% Fra her av begynner selve dokumentet
\begin{document}

% Siden NTNU-malen har en annen bakgrunn på forsida, må dette gjøres
% i en egen kommando, ikke på vanlig beamer-måte:
\ntnutitlepage
%\titlepage

% Her begynner første slide/frame, (nummer to etter forsida). 

\begin{frame}
  \frametitle{Terminology}
  \begin{tabular*}{1\textwidth}{ l l }
    \hline
    $u$ & user \\
    \hline
    $i$ & item (article, website, movie, product...)\\
    \hline
    $r$ & rating, relevance, utility (domain specific)\\
    \hline
    $m$ & a method for predicting ratings.\\
    \hline
    $p(m,u,i)$ & predicted rating of a method for $(u,i)$.\\
    \hline
  \end{tabular*}
\end{frame}

\begin{frame}
  \frametitle{2006: The Netflix Challenge}
    \begin{itemize}
      \item USD 1MM prize for a 10\% accuracy improvement.\\
      \item Breakthrough: Combining methods from many teams.\\
      \item Team BellKor finally achieved a 10.06\% improvement by combining \textbf{107} different recommender algorithms.
    \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Today: Web Search}
    \begin{block}{Google}
      "Today we use more than 200 signals, including PageRank, to order websites, and we update these algorithms on a weekly basis."
      \color{gray}{(\url{google.com/corporate/tech.html})}
    \end{block}
    \begin{block}{Bing}
      "We use over 1,000 different signals and features in our ranking algorithm."
      \color{gray}{(\url{bing.com/community/site_blogs/b/search/archive/2011/02/01/thoughts-on-search-quality.aspx})}
    \end{block}
\end{frame}

\begin{frame}
  \frametitle{Why multiple algorithms?}
  \begin{itemize}
    \item Use more data.
    \item Capture more predictive aspects.
    \item Disjoint predictors.
  \end{itemize}
  
  \begin{block}{Bell, R., Koren, Y., and Volinsky, C. (2007) (Netflix)}
    "Quite frequently we have found that the more accurate predictors are less useful within the full blend."
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{The Problem: Latent Subjectivity}
  \begin{eqnarray}
    \hat{r}_{u,i} = \sum_{m \in M} w_{m} \times p_{r}(m,u,i)
  \end{eqnarray}
  \begin{itemize}
    \item Generalized optimal weights.
    \item Treats all users and items the same.
    \item Varying accuracy across users and items.
    \item Methods are chosen by the system, not the users or items.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The Problem: Latent Subjectivity}
  \huge
  \linespread{2}{
    Systems that insist on being adaptive in a certain way
    are not really adaptive at all.  
  }
\end{frame}

\begin{frame}
  \frametitle{Adaptive Recommenders}
  \begin{eqnarray}
    \hat{r}_{u,i} = \sum_{m \in M} p_{w}(m,u,i) \times p_{r}(m,u,i)
  \end{eqnarray}
  \begin{itemize}
    \item $p_r$: predicted rating from method $m$ for $(u,i)$.
    \item $p_w$: predicted optimal weight for method $m$ for $(u,i)$.
    \item We can use standard recommenders for both $p_r$ and $p_w$.
  \end{itemize}
\end{frame}

\begin{frame}
  \input{figure.layers}
\end{frame}

\begin{frame}
  \frametitle{Training phase}
  
  \begin{itemize}
    \item Split into two sets using bootstrap aggregation: $(d_m,d_e)$.
    \item Use $d_m$ to train the basic recommenders.
    \item Create an error matrix using the basic recommenders and $d_e$. 
    \item The error matrix values are predicted errors for $(u,i,m)$.
    \item Train adaptive recommenders on the error matrix.
  \end{itemize}
  
  \begin{eqnarray}
    \forall (u,i,r) \in (d_e - d_m): E(m)_{u,i} = |r - p(m,u,i)|
  \end{eqnarray}
\end{frame}

\begin{frame}
  \begin{equation*}
     R_{u,i} =
     \begin{pmatrix}
      r_{1,1} & r_{1,2} & \cdots & r_{1,i} \\
      r_{2,1} & r_{2,2} & \cdots & r_{2,i} \\
      \vdots  & \vdots  & \ddots & \vdots  \\
      r_{u,1} & r_{u,2} & \cdots & r_{u,i}
     \end{pmatrix}
    \end{equation*}

  \vspace{1em}
  
    \begin{equation*}
     E_{u,i} =
     \begin{pmatrix}
        e_{1,1} & e_{1,2} & \cdots & e_{1,i} \\
        e_{2,1} & e_{2,2} & \cdots & e_{2,i} \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        e_{u,1} & e_{u,2} & \cdots & e_{u,i}
     \end{pmatrix}
    \end{equation*}
    
    \vspace{1em}
    \begin{center}
      Train $p_r$ with $R$ and $p_w$ with $E$.
    \end{center}
\end{frame}

\begin{frame}
  \frametitle{Prediction phase}
  
  \begin{itemize}
    \item Calculate each prediction $\hat{r}_{(u,i,m)}$.
    \item Calculate each predicted error $\hat{e}_{(u,i,m)}$.
    \item The adaptive weights are the inverses of the normalized error.
    \item Sum the weighted predictions to get the final $\hat{r}$.
  \end{itemize}  
\end{frame}

\begin{frame}
  \begin{equation}
    \label{eq:adaptive}
    \hat{r}_{u,i} = \sum_{(m_{e}, m_{r}) \in M} (1 - 
    \frac{
      p(m_{e},u,i)
    }{
      error(u,i)
    }) \times p(m_{r},u,i)
  \end{equation}
  \vspace{2em}
  \begin{equation}
    error(u,i) = \sum_{m_e \in M} p(m_e,u,i)     
  \end{equation}
\end{frame}

\begin{frame}
  \frametitle{Results}
  \begin{itemize}
    \item Calculate RMSE values for basic recommenders, simple aggregations and adaptive aggregation.
    \item Used the Movielens movie rating dataset (see paper for more details).
    \vspace{2em}
    \begin{equation}
      \mathrm{RMSE}(\hat{R},R)
      = \sqrt{\frac{
          \sum_{i=1}^{n} (\hat{R}_i - R_i)^2
        }{
          n
        }}
    \end{equation}
  \end{itemize}
\end{frame}

\begin{frame}
  \input{table.movielens}
\end{frame}

\begin{frame}
  \input{plot.movielens}
\end{frame}

\begin{frame}
  \input{plot.sigma}
\end{frame}

\begin{frame}
  \frametitle{Limitations}
  \begin{itemize}
    \item Lots of added complexity for fairly unknown improvement.
    \item Only tested on a few datasets, no real world situations.
    \item Only compared to simple aggregation methods.
    \item Neither the aggregators nor the basic recommenders were
      heavily optimized to the domain of the dataset.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Adaptive Recommenders}
  \begin{itemize}
    \item Combine disjoint algorithms
    \item Weight recommenders by predicted accuracy.
    \item Accuracy predictions are contextually dependent on $(u,i,m)$.
    \item \emph{Any} applicable recommender becomes a worthy addition.
    \item See paper for references and more results.
  \end{itemize}
\end{frame}

\end{document}