\section{Stacked Recommenders}
\label{sec:usermetamodeling}

\emph{Stacked Recommenders} (SR) is a technique for combining recommender systems
in an effort to answer two important questions.
Given that we wish to predict the relevance of an item to a user,
using many methods that consider disjoint data patterns,

\begin{enumerate*}
  \item What rating does each method predict?
  \item How accurate will each of these predictions be?
\end{enumerate*}

User modeling methods and recommender systems traditionally only care about the first question:
a single method is used to predict an unknown rating.
Modern aggregation techniques goes one step further, and combines many methods using a generic (often weighted) combination.
However, we wish to make the aggregation \emph{adaptive},
so that the aggregation itself depends on which user and which item we are considering.

Formally, we define stacked recommenders as \emph{adapting a set of recommender systems
with another complementary set of recommender systems} 
(see Figure \ref{fig:stackedusermodeling}).
The first set creates standard prediction scores, and answers the first question.
The second set predicts how accurate each method will be for the current user and item,
answering the second question.
The interesting bit is that SR can use recommender systems for both these tasks, as we shall soon see.
A system for stacked recommenders is specified by a 6-tuple:

\begin{eqnarray*}
  \mathrm{SR} &=& (Items, Users, Ratings, Framework, Methods, Adapters)\\
              &=& (I,U,R,F,M,A).
\end{eqnarray*}
%
As always, we have sets of $Users$ and $Items$, 
and a set of $Ratings$: each user $u \in U$ can produce a rating $r \in R$ of an item $i \in I$.
Items can be just about anything: documents, websites, movies, events, or indeed, other users.
The ratings can be explicitly provided by users, for example by rating movies,
or they can be mined from existing data, for example by mining query logs.
As before, we use the term "rating" loosely --- equivalent terms include \emph{relevance}, \emph{utility},
\emph{score} or \emph{connection strength}. In other words, this is a measure of what a user thinks of an item
in the current domain language. However, since \emph{rating} will match the case study we present in the next chapter,
that is what we shall use. 

\input{figure.stack.comparison}

The $Framework$ variable specifies how the data is represented.
The two canonical ways of representing users, items and ratings are graphs and matrices (see Section \ref{sec:recommender}).
We shall use a matrix, where the first dimension corresponds to users, the second to items, and each populated cell is an explicit or implicit rating:

\begin{eqsp}
 R_{u,i} =
 \begin{pmatrix}
  r_{1,1} & r_{1,2} & \cdots & r_{1,i} \\
  r_{2,1} & r_{2,2} & \cdots & r_{2,i} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  r_{u,1} & r_{u,2} & \cdots & r_{u,i}
 \end{pmatrix}.
\end{eqsp}
%
As we wish to leverage disjoint data patterns, we have a set of modeling $Methods$, 
each with their own way of estimating unknown ratings. 
Each model $m \in M$ is used to compute independent and hopefully complimentary predictions.
In our case, these methods are recommender systems.

As demonstrated in Chapter \ref{chap:theory}, there are many different recommendation algorithms,
that consider differents aspects in the data: users, items and ratings, as well as 
sources such as intra-user connections in social networks or intra-item connections in information retrieval systems.
Examples of such recommender systems include Slope One predictions, SVD factorization and Nearest Neighbor weighted predictions
(see Section \ref{subsec:recommender:examples}).
These methods predict unkown connections between users and items based on some pattern in the data,
for example user profile similarity, rating correlations or social connections.
As previously explained, to achieve the best possible combined result, we wish to use methods that look at disjoint patterns, 
i.e. complementary predictive parts of the data (see Section \ref{sec:aggregate}).

The $Adapters$ part of our 6-tuple refers to the second level of user modeling methods.
In traditional prediction aggregation this is a simple linear function for combining the different predictions,
for example by precomputing a set of weights, one for each method.
As found by \citet[p6]{Bell2007} the accuracy of the combined predictor is more dependent on the 
ability of the various predictors to expose different aspects of the data, than on 
the individual accuracy of each predictor.
As described in Section \ref{sec:aggregate}, multiple prediction results are normally 
combined into a final singular result,
based on a generalized combination found by minimizing some error across all users.

With stacked recommenders, the $Adapters$ are themselves user modeling methods 
(see Figure \ref{fig:stack:comparison}).
However, instead of modeling users, we wish to model each recommender system.
More specifically, we wish to model the \emph{accuracy} of each recommender system.
Methods in this second layer are used to predict how accurate each of their corresponding basic recommenders will be.
It is these methods that will allow us to do adaptive aggregation based on the current user and item.
In other words, we have two distinct layers of user modeling 
(see Figure \ref{fig:stackedusermodeling}):

\input{figure.stack}

\begin{enumerate}
  \item
    \emph{The methods layer} consists of traditional user modeling methods, that use a single aspect of the data to produce predictions.
    When presented with an item and a user, these methods produce a predicted rating $\hat{r}_{ui}$ based on their algorithms.
  \item
    \emph{The adaptive layer} is another set of corresponding modeling methods, that work a bit differently.
    These methods take an item and a user and estimates how well its underlying method will perform this prediction.
    The accuracy estimations are then combined with the predictions by aggregation.
    Each of these adaptive methods do not have to employ the same algorithm as their corresponding methods,
    the layers are only similar in that both consist of recommender systems.
\end{enumerate}

Another way of describing (and implementing) these two levels is through 
the $\mathrm{map}$ and $\mathrm{reduce}$ functions of functional programming.
For example, we can express prediction aggregation as 

\begin{eqsp}
  \hat{r}_{ui} = \mathrm{reduce}(u, i, \mathrm{map}(M, u, i)).
\end{eqsp}
%
First, each modeling method is applied by the $\mathrm{map}$ function, with the current user, item and set of modeling methods as input.
This operation returns a set of scalar prediction values. 
These values are then combined by the $\mathrm{reduce}$ function, which also takes the current user and item as input.
In other words, $\mathrm{map}$ is the methods layer, and $\mathrm{reduce}$ is the adaptive layer.
If we wish to do rank aggregation, the equation is a bit different:

\begin{eqsp}
  \tau_{u,n} = \mathrm{reduce}(u, \mathrm{map}(M,u,n)).
\end{eqsp}
%
Here, $\tau_{u}$ is the list of recommended items for user $u$ (following the notation in \citet[p3]{Dwork2001}).
Note that there is no input item in this formula as we wish to produce a ranking of the top $n$ recommended items.
The result is an adaptively sorted list of the top $n$ items for the current user.
A common use case for rank aggregation is personalized search:
an IR algorithm restricts the item space, which is then adapted by recommender systems,
as we shall soon see.

Expressing ourselves in terms of $\mathrm{map}$ and $\mathrm{reduce}$ is helpful 
as this will guide our implementation into a proper MapReduce framework
for parallell computation (as explained in \citet[p75]{Manning2008}).
For optimal performance, both layers can be performed in paralell, as we shall soon see.


\subsection{Adaptive Aggregation}

To perform adaptive aggregation, we need the $Adapters$ to be actual recommender systems.
Until now we have talked about both prediction aggregation (scores) and rank aggregation (sorted lists).
For now we shall stick to scalar predictions, but will return to rank aggregation in Section \ref{sec:methods:rank}.
The simplest generalized way of prediction aggregation is to take the avereage of all predictions made
by the different methods (e.g. \citet[p3]{Aslam2001}):

\begin{eqsp}
  \hat{r}_{ui} = \frac{1}{N} \sum_{m \in M} p(m,u,i).
\end{eqsp}
%
Here, $\hat{r}_{ui}$ is the estimated rating from user $u$ to item $i$,
$N$ is the number of methods in $M$, and $p(m,u,i)$ is the predicted rating from method $m$.
However, many aggregators attempt to weigh each method differently (e.g. \cite{Claypool1999}):

\begin{eqsp}
  \hat{r}_{ui} = \sum_{m \in M} w_{m} \times p(m,u,i) 
  \quad \text{where} \quad 0 \leq w_{m} \leq 1, \quad \sum_{i \in M} (w_i) = 1.
\end{eqsp}
%
In this equation, $w_m$ is the weight applied to modeling method $m$. 
These weights fall in the range $[0,1]$ and sum up to $1$.
As described in \ref{sec:aggregate}, the weights can be estimated through different machine learning methods.
However, as discussed in Section \ref{sec:reasoning},
this is still a generalized result, averaged across every user. 
The system assumes that the best average result is the best result for each individual user.

In order to leverage as many patterns as possible, and remove the latent subjectivity,
we need \emph{adaptive weights} that are computed spcifically for each combination of a user and an item.
However, if we wish each weight to be combination-specific, pre-computing each weight for each method becomes unfeasible.
In other words, these adaptive weights have to be estimated just as the ratings themselves:

\begin{eqsp}
  \hat{r}_{ui} = \sum_{m \in M} p_{w}(m,u,i) \times p_{r}(m,u,i)
  \quad \text{where} \quad
  \sum_{m \in M} (p_{w}(m,u,i)) = 1.
\end{eqsp}
%
We have now reduced our mission to creating a method that can estimate the adaptive weights: $p_{w}(m,u,i)$.
As mentioned, we wish to use standard recommender systems for both of these predictions.
For recommender systems to be applicable here, we need to create a matrix (or graph)
that stores known estimations of how accurate some of the predictions will be.

\input{figure.errors}

The key insight is that \emph{the accuracy of a method is the inverse of its predicted error}.
By modeling the errors of a method through standard recommender systems,
we can in turn predict errors for untested combinations
(see Figure \ref{fig:adaptiveweights}).
Consider the following \emph{error matrix}:

\begin{eqsp}
 E_{u,i} =
 \begin{pmatrix}
    e_{1,1} & e_{1,2} & \cdots & e_{1,i} \\
    e_{2,1} & e_{2,2} & \cdots & e_{2,i} \\
    \vdots  & \vdots  & \ddots & \vdots  \\
    e_{u,1} & e_{u,2} & \cdots & e_{u,i}
 \end{pmatrix}
\end{eqsp}
%
Creating an error matrix for each modeling method is quite simple:
by splitting the ratings data in two,
the first set can be used for the actual training, and the second
can be used to populate each error matrix.
Each standard modeling method gets an error matrix where some cells have values:
each value corresponds to the prediction error for a combination of a user and an item.
Notice how similar this matrix is to the previously introduced ratings matrix.
This similarity is what will allow us using the exact same modeling methods
to perform adaptive aggregation.
Whenever we wish to train a new modeling method,
\emph{the modeling phase}, we apply the following algorithm:

\begin{enumerate*}
  \item Split the ratings data into two sets for training and error estimation.
  \item Train the modeling method in its specific way with the first training set.
  \item Use the error estimation data set to create the error matrix.
  \item Train a meta modeling method (error model) based on the error matrix.
\end{enumerate*}

When we have an error model for each modeling method, 
we can use these errors to estimate each weight.
Whenever we wish to create an adaptive aggregate prediction,
\emph{the prediction phase},
we apply the following algorithm:

\begin{enumerate*}
  \item Collect predictions from each modeling method for $(u,i)$.
  \item Collect estimated errors for each method for $(u,i)$.
  \item Normalize the errors so that the error vector sums to $1$.
  \item Compute a weighted combination where each weight is $1 - e_{m,u,i}$.
\end{enumerate*}

The next section will explain these steps in detail.
For now, we have our approach to stacking recommenders:

\begin{eqsp}
  \hat{r}_{ui} = \sum_{(m_{e}, m_{r}) \in M} (1 - p(m_{e},u,i)) \times p(m_{r},u,i),
\end{eqsp}
%
where $p$ returns a prediction from a model for a specific user and item,
$m_{r}$ is a standard recommender model, 
and $m_{e}$ is its corresponding error model.
For this equation to work as expected, the weights must be normalized:

\begin{eqsp}
  0 \leq p(m_e,u,i) \leq 1 \quad \text{and} \quad \sum_{m \in M} (p(m_e,u,i)) = 1.
\end{eqsp}
%
Notice that the \emph{only} difference between $m_e$ and $m_r$ is how they are created.
$m_r$ is trained with the standard ratings matrix, and $m_e$ is trained using the error matrix.
This means we can use \emph{any} standard modeling method to perform adaptive aggregation.
Hence, the name \emph{stacked recommenders}: 
each standard modeling method is stacked under another accuracy estimating modeling method.

It is also important to note that the types of recommenders used for the adaptive layer
is independent of the basic recommenders.
Each adaptive recommender need only predicted ratings from each basic recommender,
and does not care which algorithm it employs.
When making predictions, the calculations in the methods layer and adaptive layer
are independent of each other, as both use pre-computed models:
the method layer use the ratings matrix, or their own models
created during training, while the adaptive layers use the error matrices for each
basic method.

The result of this is a system that does not only aggregate a number of predictions for each unknown
combination of users and items,
but that also combines these methods based on how accurate each prediction is likely to be.
Now that we have our model, it is time to see how it can be implemented.
First, we shall do prediction aggregation in a recommendation scenario,
and then rank aggregation in an information retrieval scenario.

