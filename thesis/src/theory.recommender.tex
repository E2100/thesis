\section{Recommender Systems}

The name might seem constraining, but recommender systems are incredibly powerful methods in user modeling.
Whenever we wish to predict the relevance of an item to a user, recommender systems are the tools to use.
Such systems are commonly used on the web to provide a host of predictive functionality, including:

\begin{itemize*}
  \item Recommending products like books or movies based on past purchases.
  \item Suggesting new social connections based on an existing social graph.
  \item Recommending items based the activity of similar or like-minded users.
  \item Ordering news articles by predicted individual relevance.
\end{itemize*}

Common to these examples are a set of users, a set of items, and a sparse set of explicit ratings or preferences.
A recommender system is best described as a graph, even though the underlying algorithms might not use this as the representation.
\cite{Mirza2003} explains how any RS can be expressed as a graph traversal algorithm.
Items and users are nodes, while ratings, social connections et cetera are edges between the nodes.
An RS performs predictive reasoning on this graph by estimating the strenghts of hypothetical connections between nodes that are not explicitly connected.

For example, if a user has rated some of the movies in a movie recommendation system, 
we use these ratings to predict how well the user will like unseen movies,
based on a movies ratings from users similar to the one in question.
In social networks, recommender systems can be used to infer new social relations 
based on existing connections. The principle is the same: By evaluating current explicit
connections, and the connections of similar users, new connections can be predicted.
Recommender systems are then powerful methods for user modeling, personalization and fighting information overload,
because of their ability to infer how relevant and item (or another user) will be to the current user.

Formally, a recommender system is a quintuple, $\mathrm{RS} = (I, U, R, F, M)$,
where $I$ is the set of items (e.g. products, articles or movies) and $U$ is the set of users.
$R$ is the set of known ratings, i.e. explicit preferences given by users for certain items.
$F$ is a framework for representing the items, users and ratings, and 
$M$ is the actual modeling method used to infer unknown ratings 
for predicting a user's preference for an unrated item. 

In \cite{Adomavicius2005}, $M$ is seen as a utility function
$f: U \times I \rightarrow S$. Here, $f$ is a function that maps the set
of users and items into a fully ordered set of items $S$, ranked by their
utility (i.e. rating) to each user. In other words, $S$ is the complete version of $R$,
where each user has either an explicit or predicted preference for each item in $I$.
In this notation, to predict the best unrated item for each user, we simply find the item with the highest expected utility:

\begin{eqnarray*}
  \forall u \in U,\text{ } i'_u = \arg\max_{i \in I} f(u,i)
\end{eqnarray*}

The utility function $u$ depends on the modeling method being used, the active user and the item in question. 
The \emph{reason} for using a recommender system is that the utility $u$ is not defined for the entire $U \times I$ space, 
i.e. the system does not explicitly know the utility of each item for each user. 
The point of a recommender system is then to extrapolate $u$ to cover the entire user-item space. 
In other words, to be able to rank items according to user preferences, 
the system must be able to predict each user's reaction to items they have not yet explicitly rated themselves. 
This is where predictive user models come in handy.

Another popular way of describing, and implementing an RS is using a simple matrix. 
Here, one dimension represents users, the other dimension represents items,
and each cell corresponds to an explicit rating. This matrix then becomes the framework $F$ in our 
RS quintuple:

\begin{eqnarray*}
 R_{m,n} =
 \begin{pmatrix}
  r_{1,1} & r_{1,2} & \cdots & r_{1,n} \\
  r_{2,1} & r_{2,2} & \cdots & r_{2,n} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  r_{m,1} & r_{m,2} & \cdots & r_{m,n}
 \end{pmatrix}
\end{eqnarray*}

Critically, these matrices are usually extremely sparse (i.e. most of the cells are empty). 
Consider that while there may be a large number of users and items, each individual user
only rates or connects to a few number of items. 
For example, in the seminal Netflix Challenge movie recommender dataset, almost 99\% of the potential
user/item pairs have no rating \citep[p1]{Bell2007d}. In other words, the recommender system must be able
to produce results from a matrix where only 1\% of the cells have meaningful values.

Naturally, this is the defining characteristic of 
many recommender systems: the ability to extract meaningful patterns from sparse data, 
through dimensionality reduction, neighborhood estimation and similar methods.

Recommender systems face many challenges other than the sparsity problem.
A directly related problem is the need for large datasets. Since the data is often sparse,
the systems will most often perform well if used on large numbers of items and users.
As in many machine learning methods, concept drift, where the characteristics of a user or item
changes over time, is also eternally present.
Finally, the performance of RSs is often closely tied to their computational complexity. 
Real world usage of the most precise methods is often hindered by the computational power
needed to actually put them into production.




\subsection{Estimation of Ratings}

The most interesting and important part of any RS is how it predicts unknown ratings.
(Note that altough we use "ratings", "utility", "preference", "relevance" and "connection strength" depending on the context, they all basically mean the same.)
Because of this, each method is best categorized based on a few dimensions of its predictive capabilities (see Table \ref{table:taxonomy}).
In our taxonomy, these dimensions are: Predictions, method, granularity, temporality and agents.

\begin{table}[b]
  \begin{tabular*}{\textwidth}{ p{3cm} l @{\extracolsep{\fill}} }
    \toprule
    \emph{Variable} & \emph{Values} \\
    \midrule
    Predictions & Content-based | Collaborative | Hybrid\\
    Method & Heuristic | Model-based\\
    Granularity & Canonical | Typical | Individual\\
    Temporality & Short-term | Long-term\\
    Agents & Implicit | Explicit\\
    \bottomrule
  \end{tabular*}
  \caption[Recommender Systems Taxonomy]{A taxonomy of recommender systems. From \cite{Bjorkoy2010d}.}
  \label{table:taxonomy}
\end{table}

The \emph{predictions} variable represents what data the RS uses to perform predictions. 
Content-based methods use only the items, inter-item relations, and 
an individual user's past history as predictive of future actions \citep{Pazzani2007}.
By only considering the individual user in adapting an application, highly personal models can be created. 
However, such methods often require a lot of interaction before reliable models can be created \citep{Adomavicius2005}.
The problem of having to do complex inference from little data, as is often is in content-based predictions, is often called the \emph{sparsity problem} or the \emph{cold start} problem. This is closely related to the problem of \emph{overfitting} data, where the algorithms creates models that match the training data, but not the actual underlying relationships. A lot of research looks at ways to overcome sparse data, i.e. achieving "warmer" cold start. 
When using content-based predictions, the utility function $f(u,i)$ of user $u$ and item $i$ is extrapolated from $f(u,i_u)$, 
where $i$ is an item similar to $i_u$ and $f(u,i_u)$ is known.

Collaborative or social recommendations build predictive models for users based on the actions of similar users 
\citep{Schafer2007}.
The observation is that similar users should have similar usage and action patterns. 
By using data from more than one user, expansive models may be built. 
These methods are especially useful when considering new users of a service. 
A central problem with collaborative methods is that the resulting model is not as individually tailored as one created through content-based learning. 
Collaborative models must be careful not to represent the \emph{average} user, but a single individual.
When using collaborative learning, 
the utility $f(u,i)$ of item $i$ for user $u$ is extrapolated from $f(u_j,i)$ where $u_j$ is a user similar to $u$. 

Because of \emph{the new user problem} of content-based learning and the \emph{average user problem} of collaborative learning, 
many systems use a hybrid approach \citep{Burke2007}.
By combining content-based and collaborative learning, 
systems that properly handle predictions for new users and avoid too much generalization in the models can be achieved. 

The \emph{method} variable, is another way to classify recommenders. Orthogonal to what data the method uses, this variable
concerns \emph{how} the data is used to produce recommendations.
First we have the \emph{model-based} approach, where the recommender system builds predictive models based on the known data. 
Unseen items can then be fed into this model to compute its estimated utility score. 
For example, creating a Bayesian networks from past interaction is a model-based approach.
The other category is the \emph{heuristic} or \emph{memory-based} approach. 
These methods use the raw data of items, users and ratings to directly estimate unknown utility values. 
For example, recommending items similar to the ones already rated by computing the cosine similarity of their feature vectors is a heuristic approach.


The \emph{granularity} variable tells whether this approach creates models for the canonical user, stereotypical users or individual users. 
\cite{Rich1979} presented one of the first user modeling systems based on stereotypes, used to predict which books in a library each user would most enjoy.
Here, a dialogue between the system and the user was performed to place the user into a set of sterotypes. 
Each stereotype has a set of \emph{facets} which is then used to match books and users.

\emph{Temporality} refers to how volatile the gathered knowledge will be.
While most RSs produce long term, relatively stable knowledge based on lasting user preference and taste, 
some systems use fluctuating parameters such as the time of day, exact location and the current context to produce recommendations.
For example, \cite{Horvitz} used clues from a user's calendar, camera and other sensors to determine the attentional state
of the user before delivering personalized and contextual notifications.

The \emph{agents} variable signifies whether the knowledge gathering and presentation is implicit and opaque, 
or explicit and requires dedicated user interaction. Explicit feedback through ratings is 
common in movie, product or music rating services (e.g. \cite{Bell2007, Basu1998, Hotho}). However, for other services such as personalized search,
implicit mining of query logs and user interaction is often used to build user models (e.g. \cite{Shen2005, Agichtein2006, Speretta2000, Teevan2005})


\subsection{Approaches}

Because our solution will combine different recommender systems, we need a short introduction to some of the approaches we will combine.

TODO
