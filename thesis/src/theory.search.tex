\section{Personalized Search}
\label{sec:search}

Personalized search means adapting the results of a search engine to each individual user.
As we shall see, this field has a lot in common with user modeling and recommender system.
In both situations, we wish to predict how relevant an item will be to each user.
Before delving into the techniques of personalizing search results, we present 
the basics of \emph{information retrieval} (IR).

\subsection{Information Retrieval}
\label{sec:ir}

\citet[p1]{Manning2008} define IR as "finding material (usually documents) of
an unstructured nature (usually text) that satisfies an information need
from within large collections (usually stored on computers)".

How does this relate to recommender systems? There is an important distinction:
The purpose of \emph{recommending} is twofold: (1) show the user items
similar to another item, and (2) allow discovery of relevant items the user did not know exist.
The purpose of \emph{search} is a bit different: allow the user to find the location of
information he or she knows (or hopes) exists.
In other words, the defining separator is often the knowledge of existence.

However, as we shall see in this chapter, the two fields employ a lot of the same
methods and terminology. In the next chapter, we will show how these can work together.

\citet[p23]{Baeza-Yates1999} presents a formal definition of an IR system:
$\mathrm{IR} = (Documents, Queries, Framework, ranking(q_i, d_i))$

As evident by the scope of IR literature, these elements can be just about anything
that has to do with retrieving information. However, in what is often called
\emph{classic IR}, the documents contain free text with little to no describing structure,
and the queries are short user-initiated descriptions of an \emph{information need} \citep[p19]{Baeza-Yates1999}. 
Among other domains, this model describes web search engines, where the documents are web pages and
queries are short sentences or a few keywords input by users.

The \emph{Framework} in our quadruple refers to how documents are stored and retrieved.
Basic approaches to IR split each document into a set of terms (e.g. words),
and create an inverted index \cite[p22]{Manning2008} that lists each document that each term appears in.
There are numerous extensions to this framework, including: 

\begin{itemize*}
  \item Positional information for phrase search \cite[p39]{Manning2008}
  \item Stop word removal (removing the most common terms) \cite[p27]{Manning2008}
  \item Stemming (reducing words to their root forms) \cite[p32]{Manning2008}
  \item Lemmatization (contextual inflection removal) \cite[p32]{Manning2008}
  \item Query reformulation \citep[p117]{Baeza-Yates1999}
\end{itemize*}

All these techniques help improve (among other things)
the \emph{recall} and \emph{precision} of the retrieval engine. 
Recall, precision and relevance are well defined masures for evaluating the quality of a search engine \cite[p5]{Manning2008}:

\begin{itemize*}
  \item A document is \emph{relevant} if it satisfies the user's information need.
  \item \emph{Recall} is the fraction of relevant documents retrieved by the system.
  \item \emph{Precision} if the fraction of retrieved documents that are relevant.
\end{itemize*}

There are many more measures, but recall and precision succintly define what a search engine must to
to be successfull: retrieve many relevant documents and few irrelevant documents.
Failing this test is to neglect the main purpose of an IR:
preventing information overload by allowing people efficient access 
to relevant parts of an otherwise overwhelming information repository.

To us, however, the most interesting part of any IR system is the \emph{ranking function}.
This function maps queries to documents by a scalar score, signifying how well a match
each document is to a query. The relation to recommender systems should be self-evident,
and indeed, IR systems use many of the same metrics to measure query/document similarity.

A common framework for storing and ranking documents is the previously mentioned vector space model (VSM).
This model stores documents as term vectors. Each term represents a dimension, and documents are
vectors in this term-space. When performing a query, the query terms are also represented as a vector
in the same space. By computing the cosine similarity between the query and each document,
we get a good estimate of how well a document matches a query \citep[p29]{Baeza-Yates1999}.

The next question is what to store at each (document, term) coordinate in the vector space
(called the document-term weights).
Storing simple 1 or 0 values representing whether or not terms are present gives a model 
where a document's relevance is proportional to how 
many of the query terms it includes. However, this is not very precise. 
For example, by this definition, a document containing every conceivable query term
would be the most relevant to any query.
A better idea is to use something like the TF-IDF weighting scheme \citep[p29]{Baeza-Yates1999}:

\begin{equation*}
  w_{t,d} = tf_{t,d} \times idf_{t}
          = \frac{ \mathrm{freq}(t,d) }{ \sum_{k \in d} freq(k,d) } \times 
            \log \frac{N}{n_{t}}.
\end{equation*}

The final weight is computed by multiplying the term frequency score (TF) $tf_{t,d}$ with the 
inverse document frequency (IDF) $idf_{t}$. TF evaluates how well the term describes the document contents,
while IDF punish terms that appear in many documents. 
$freq_{t,d}$ gives the frequency of a term in a document. $N$ is the total number of documents,
and $n_{t}$ the number of documents in which $t$ appears. The effect of the IDF factor is dampened by taking its
log-value. Together, TF and IDF ranks documents higher by words that discriminate well within the document corpus,
and ignores words that appear in so many documents that they have little to no predictive capacity.

While simple, TF-IDF has proven itself resilient when compared to more complex methods,
and many more complex methods have been built on its foundations (e.g. BM25, one of the most successfull
probabilistic weighting algorithms \citep{Robertson2010}).

There are as many IR models as there are domains that need search,
and even the basic vector space model can be constructed in a myriad of ways. There is also the simpler 
\emph{boolean search model}, where queries are based on boolean algebra. Probabilistic models
frame the similarity question as the probability that the document is relevant. 
Latent Semantic Indexing (LSI), the application of SVD to IR by performing dimensionality reduction of the term-space
into concept-space is another approach.
See \cite{Manning2008}, \cite{Robertson2010} or \cite{Baeza-Yates1999} for a more comprehensive introduction to models in IR.

The important take-away is that, while serving different use cases, RSs and IR systems 
employ much of the same technology with different input and expected output.


\subsection{Ranking Signals}
\label{subsec:signals}

Modern web search engines have long ago moved on from simple ranking metrics such as TF-IDF.
While similar traditional metrics may form the foundation of modern search engines, a lot more thought go into the final results.
Different types of rankings are combined to produce the final \emph{search engine results page} (SERP),
with each ranking function often being referred to as a \emph{signal}. Alternate names include
\emph{reranking} or \emph{rescoring} functions.

Google, the company behind the popular online search engine, writes: "Today we use more than 200 signals, including PageRank, 
to order websites, and we update these algorithms on a weekly basis. 
For example, we offer personalized search results based on your web history and 
location."\footnote{\url{google.com/corporate/tech.html} --- accessed 11/04/2011}
Bing, another popular search engine, uses the same terminology:
"We use over 1,000 different signals and features in our ranking 
algorithm."\footnote{\url{bing.com/community/site_blogs/b/search/archive/2011/02/01/thoughts-on-search-quality.aspx} --- accessed 11/04/2011}

Signals are often products of the document structure of the current domain.
\citet[p5]{Bender2005} points to the use of the proximity of query terms in matching documents.
Those where the terms appear close together are natural candidates for a higher ranking.
Other signals, still reliant on the documents themselves, are more domain oriented.
Another signal they point out is how words in a larger or bold font can be weighted 
higher than normally typset words.

Signals can also depend on the query. \citet[p145]{Manning2008} describes a system that takes
multi-word queries, breaks them up into different permutations and runs each new query against the same
document index and ranking function. Each query corresponds to its own ranked set of results,
which are sent to a \emph{rank aggregation function} which turns the accumulated ranking evidence
into one coherent result. We will have more to say on rank aggregation in Section \ref{sec:aggregate}.  

Signals can also be external to the collection or relational within the collection.
PageRank \cite[p4]{Bender2005} is perhaps the most known of the relational signals.
The algorithm forms a probability distribution over web pages, ranking their percieved
authority or importance according to a simple iterative estimation.
Each web site is given its rank based on how many pages that link to it.
For each page that provides links, the score it contributes to the linked-to page is 
its own page rank weighted inversely proportional to the number of outbound links the page has.
Another intuitive justification for a site's PageRank is the \emph{random surfer model} \cite[p4]{Bender2005}.
The probability that the random surfer visits a page is its PageRank. The "randomness" is introduced 
by a damping parameter $d$, which is the probability that a user will stop browsing and start at a new random page:

\begin{equation*}
  \mathrm{PageRank}(x) = \frac{1 - d}{N} + d \sum_{y \in B_x} \frac{\mathrm{PageRank}(y)}{\mathrm{Links}(y)},
\end{equation*}

where $B_x$ is the set of pages linking to page $x$, and $\mathrm{Links}(y)$ is the number of outbound links from page $y$.
The first term distributes an equal pagerank score to all pages that have no outbound links, as $N$ is the total number of pages.
This iterative algorithm is run until convergence inside a small delta.

Let us now finally take a look at one of the main uses of signals: \emph{personalized search}.


\subsection{Personalizing Search Results}

Search engines, especially online search engines, face a huge challenge. 
In addition to the wide range of websites, the ambiguity of language,
the restricted nature of queries, comes the wildly differing users.
Each user is unique. Even when considering one user, there might be many 
different use cases, for example when using the same search engine at work and at home.
Another identified problem is that users use search engines for navigation as well as pure search.
\citet{Teevan2007} found that as many as 40\% of all queries to the Yahoo! search engine were "re-finding queries",
i.e. attempts to find information the user had accessed before.

\emph{Personalized search} (PS) attempts to solve these problems by introducing individually catered search results. 
These techniques are based on user modeling (as introduced in Section \ref{sec:modeling}),
and attempts to build predictive models based on mined user preferences.
Commonly, this is done through query log analysis (e.g. \cite{Liu2002, Sugiyama2004, Shen2005, Speretta2000})
In other words, these are often model-based techniques with implicit knowledge gathering agents,
that create individual, long-term user models (see Section \ref{sec:recommender}).

There are two leading approaches to personalizing search results \cite[p2]{Noll2007}. 
The first method is query reformulation, where the actual user query is enhanced in some way, before traditional IR 
retrieves and ranks documents. The second method is results re-ranking, where the IR results are sorted
based on personalized metrics. This section describes the latter approach.

To demonstrate how these methods compare to traditional recommendation systems,
we will explore a few different approaches to personalized search: 
(1) \emph{personalized topic-sensitive PageRank},
(2) \emph{folksonomy-based personalization} and
(3) \emph{social network search ranking}.

(1) \citet{Haveliwala2003} introduced a topic-sensitive PageRank algorithm, that they found
to be "generate more accurate rankings than with a single, generic PageRank vector". 
In essence, they create topic-specific PageRank vectors for a number of pre-set topics,
creating many rankings per page, one for each topic.
This new PageRank is computed based on an existing set of websites that belong to each topic.
\citet{Qiu2006} achieved "significant improvements" to this approach by adding a personally adaptive layer
to the topic-sensitive PageRank algorithm, creating a \emph{personalized PageRank algoriithm}. 

In addition to the topic vector, \citeauthor{Qiu2006}
creates a topic-preference vector for each user. When the user has clicked on a few search results,
a learning algorithm kicks in and estimates approximately how likely the user is to be interested 
in each of the pre-set topics, creating the topic-preference vector $T$. When it is time to rank a 
page $p$ in response to the query $q$, they compute the personalized ranking:

\begin{equation*}
  PersonalizedRanking(T,p,q) = \sum_{t=1}^{m} T(i) \cdot Pr(q|T(i)) \cdot TSPR_i(p)
\end{equation*}

We will not deduce this equation here (see \citet[p5]{Qiu2006}), but let us explain it. 
$T$ is the user-specific topic preference vector.
$i$ is the index of a topic and $m$ the total number of topics.
$Pr(q|T(i))$ is the probability that the query belongs in topic i.
This can be as simple as the total number of times the query terms appear in websites under topic $i$.
$TSPR_i(p)$ is the topic-sensitive PageRank score for page $p$ in topic $i$. Basically, this is 
the normal PageRank vector computed within a pre-set topic $i$.

The construction of $T(i)$, i.e. the training phase of the algorithm, is performed by mining the query logs for each user.
By identifying how many sites the user has visited in each topic, computing $T$ can be done through linear regression or
by using a Maximum-likelihood estimator (basically, any method that can fit a curve).
\citet[p10]{Qiu2006} reports improvements of 25\% to 33\% over the Topic-sensitive PageRank approach, which 
\citet{Haveliwala2003} reports outperformed the original PageRank algorithm.

%Cube svd: \cite{Sun2005}

(2) Web applications often have more information about users and items (documents, sites or articles) 
than simple ratings. One of these extra resources are tags, simple keywords assigned from users to items. 
The collection of users, items, tags and user-based assignment of tags to resources is called a \emph{folksonomy}.

\cite{Hotho} defines a folksonomy as a tuple $F = (U,T,R,Y,\prec)$. 
Here, $U$, $T$ and $R$ are finite sets of users, tags and resources (items), respectively. 
$Y$ is a ternary relation between users, tags and resources, called tag assignments. 
$\prec$ is a user-specific tag hierarchy, applicable if the tags are organized as super- and sub-tags. 
The \emph{personomy} $P_u$ is a user-specific part of $F$, 
i.e. the tags, items and assignments related to one user $u$. 
In our terms, this personomy would be the user model. 
\citeauthor{Hotho} use folksonomies to do information retrieval based on their 
\emph{FolkRank} search algorithm, a derivative of PageRank. 

\cite{Bao2007} shows how folksonomies can be used to personalize search.
They first create a topic-space, where every user and document are represented.
Each tag in the system is a dimension in this topic-space, or tag-space.
Whenever a new query is issued, two things happen: First, a regular IR method
computed a standard, non-personalized ranking of documents.
Second, a personalized ranking list is computed by performing a simple
vector-space model matching in the topic-space, for example by using
cosine similarity (as previously explained). The personalized list
is then unrelated to the actual query, and is simply a ranking of the
most relevant pages to the current user.

The two ranks are aggregated by a simple consensus-metric, the
\emph{weighted borda-fuse} (WBS) aggregator \cite[p3]{Xu2008}, 
which is nothing more than a weighted combination of the rankings:

\begin{equation*}
  \mathrm{rank}(u,q,p) = \alpha \cdot \mathrm{rank}_{IR}(q,p) 
                 + (1-\alpha) \cdot \mathrm{rank}_{personal}(u,p)
\end{equation*}

\citeauthor{Xu2008} tried many combinations of weights,
topic selection and datasets, with the general conclusion
that folksonomy-based personalized search has great potential.
If nothing else, this example shows how easily tags can be integrated
to provide an individual searching experience.


%\begin{center}
%\begin{tikzpicture}[node distance   = 4 cm]
%  %\useasboundingbox (-1,-1) rectangle (11,11); 
%  \tikzset{VertexStyle/.style = {shape          = circle,
%                                 ball color     = black,
%                                 text           = white,
%                                 inner sep      = 2pt,
%                                 outer sep      = 0pt,
%                                 minimum size   = 24 pt}}
%  \tikzset{EdgeStyle/.style   = {thick,
%                                 double          = black,
%                                 double distance = 1pt}}
%  \tikzset{LabelStyle/.style =   {draw,
%                                  fill           = white,
%                                  text           = red}}
%  \node[VertexStyle](A){A};
%  \node[VertexStyle,right=of A](B){B};
%  \draw[EdgeStyle](A) to node[LabelStyle]{2} (B);
% 
%  \end{tikzpicture}
%\end{center}


(3) \cite{Carmel2009} developed a personalized search algorithm based on a user's \emph{social network}.
By re-ranking documents according to their relation to with individuals in the current user's social network,
they arrived at a document ranking that "significantly outperformed" non-personalized social search \cite[p1]{Carmel2009}.
Note, however the qualifier "social search" --- their project searches through social data within an enterprise, 
naturally conducive to algorithmic alterations based on social concepts. However, as social data is data just as well,
seeing how a personalized approach improves standard IR in this domain, is helpful.

Their approach: First, documents are retrieved by a standard IR method. Second, the user's socially connected peers
are also retrieved. Third, the initial ranked list of documents is re-ranked based on how strongly they are connected 
to the user's peers, and how strongly those peers are connected to the user. The user-user similarity is
computed based on a few signals \cite[p2]{Carmel2009}, e.g. co-authoring of documents, the use of similar tags
(see (2)) or commenting on the same content. 
The user model also includes a list of terms the current user has employed in a social context (profile, tags, et cetera).
This is all done to infer implicit similarity based on social connections.

The algorithm is quite powerful, and combines three types of rankings: 
The initial IR score, the social connection score, and a term score, where the terms are tags and keywords used by a user.
The user model is $U(u) = (N(u), T(u))$, 
where $N(u)$ are the social connections of $u$ and $T(u)$ the user's profile terms.
First, the score based on connections and terms is computed, weighted by $\beta$ which determines the weighting of both approaches:

\begin{equation*}
  S_P(q,d|U(u)) = \beta \sum_{v \in N(u)} w(u,v) \cdot w(v,d) + (1-\beta) \sum_{t \in T(u)} w(u,t) \cdot w(t,d)
\end{equation*}

Finally, the results are combined with the ranking returned by the IR method ($R_{IR}$). 
A parameter $\alpha$ is used to control how much each method is weighted:

\begin{equation*}
  S(q,d|P(u)) = \alpha \cdot R_{IR}(q,d) + (1-\alpha) \cdot S_P(q,d|U(u)) 
\end{equation*}

This approach, while simple, shows how social relations and social annotations can easily be used to personalize a search experience.
However, \citet[p10]{Carmel2009} notes that the high quality data in their enterprise setting were important
to achieve the improved results. 



