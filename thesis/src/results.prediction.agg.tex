\section{Prediction Aggregation}

\input{table.movielens}

\input{table.jester}

Our first hypothesis, H1, states that:
{
  \itshape
  the accuracy of relevance predictions can be improved
  through adaptive recommender aggregation.
}
The second hypothesis, H2, states:
{
  \itshape
  adaptive aggregation can achieve higher accuracy than global and generalized aggregation methods.
}

In order to verify these hypotheses, we performed adaptive prediction aggregation
through adaptive recommenders on the two datasets previously described.
5-fold cross validation was performed to further verify each result.

Table \ref{table:results:e1} gives the results from Experiment 1 (MovieLens).
Table \ref{table:results:e2} gives the results from Experiment 2 (Jester).
Each cell corresponds to the RMSE values for each dataset,
for each recommender and aggregation approach.
The bottom entry in this table refers to our adaptive recommenders method.
As seen in this table, the adaptive recommender achieved
lower RMSE values than any of the other applied methods.

Statistics for each experiment are given in the last
parts of Tables \ref{table:results:e1} \emph{\&} \ref{table:results:e2}. 
The statistical values are the minimum, maximum and mean values
for each of the methods. We also include
the standard deviation ($\sigma$) for each method,
across each collection of subsets.
This table confirms the results from the full results table:
Our adaptive recommenders approach improves the mean performance
of our system.
The mean performance in Experiment 1, along with the standard deviation
are shown in Figure \ref{plot:rmse}.

\input{plot.movielens}
%\input{plot.rmse}

\input{plot.sigma}

Let us take a look at the standard deviation measures from the different methods.
As seen in Figure \ref{plot:rmse}, 
most of the methods, including the adaptive models,
exhibit quite a lot of variation in their results.
If these variations occured as a result of unstable
predictions of the same dataset, this would be a substantial problem,
resulting in unreliable predictions.
However, as seen in Figure \ref{plot:datasets}
(based on Experiment 1),
the standard deviation is mostly caused by the differing
performance across the varying datasets.
As we see, the performance of each of the aggregation methods,
as well as the best performing standard recommender,
follow each other closely. At the same time,
performance varies across the different datasets,
which results in high values for $\sigma$.

What does this mean for hypotheses H1 and H2?
Expressed in terms of this experiment,
H1 posits that adaptive recommenders should outperform each of the standard modeling methods
in Table \ref{table:results:methods}.
The adaptive methods blend the results of multiple predictors by estimating the accuracy
on a per-item and per-user basis, satisfying the formulation of H1.

By outperform we mean that our model should have a lower
mean RMSE score than the other singular methods. As we can see in Tables \ref{table:results:e1} \emph{\&} \ref{table:results:e2},
\emph{H1 is confirmed for these methods and these datasets}.
While we can not generalize too much on this basis, 
the fact that this dataset is a common testing ground for recommender systems,
that RMSE is the de facto measure for determining performance,
and because of our 5-fold cross-validation, the results allow us 
to confirm hypothesis H1 in these conditions, and likely for other, similar scenarios.
We shall discuss this in Chapter \ref{chap:discussion}.

Similarly, expressed in the same terms, H2 posits that 
our adaptive recommenders should outperform the aggregation approaches
given in Table \ref{table:results:methods}.
The \emph{median} and \emph{average} aggregation methods
serve as globalized and generalized aggragation methods,
adaptive recommenders are adaptive in that each prediction is 
aggregated based on the current user and item,
satisfying the language of H2.

As we can see in Tables \ref{table:results:e1} \emph{\&} \ref{table:results:e2},
\emph{H2 is confirmed for these methods and these datasets}.
However, as our collection of aggregation methods is a lot simpler
than our collection of recommender systems, the strength of this combination
is notably weaker than that of H1.
Still, the fact that a adaptive recommender outperforms these simple aggregation
approaches is a positive result warranting further experiments.
This will also be discussed in Chapter \ref{chap:discussion}.

Our system performs better in Experiment 1 than Experiment 2.
While better performance in Experiment 2 would have been desirable,
the results fit our original assumptions.
The Jester dataset, used in Experiment 2, have very few items (100).
This would intuitively mean that there ae fewer disjoint
patterns for the adaptive layer to leverage.
As described in the previous chapter, adaptive recommenders
are mostly mean for scenarios where we have a wide range of
different users and items.
However, as in Experiment 1, our method outperforms the standard and aggregate recommenders,
if only by a small margin.

It would seem then that, based on our experiments, available data
and assumptions of evaluation measures, both H1 and H2 are confirmed.
Our adaptive aggregation approach outperforms both standard recommender
methods and simple generalized aggregation methods.
Notably, our approach is more complex than the methods it outperforms,
so the question whether the methods performance is worth its extra complexity becomes important.
We shall discuss this, and other implications and limitations of these results in the next chapter.
For now, let us proceed to the second experiment and hypothesis H3.

