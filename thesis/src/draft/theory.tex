\section{User Modeling}

\emph{User Modeling (UM)} is about adapting an application to its users.
Whenever an application behaves or presents differently based on knowledge
of individual or groups of users, it is performing user modeling. 
Examples of UM include:

\begin{itemize*}
  \item Translating content based on user location.
  \item Suggesting interesting items based on previous activity.
  \item Reorganizing content based on predicted user relevance.
  \item Changing presentation to match user preferences or abilities.
  \item Any form of personalized content, presentation or behaviour from the application.
\end{itemize*}

Naturally, with such a vague definition, user modeling spans many fields,
approaches and methods. However, there are two core problems each method tries to solve:

\textbf{Information Overload} occurs when an application presents more information than a user is able to consume.
\cite{Bjorkoy2010d}
There are plenty of potential culprits: a poor signal to noise ratio, where relevant content is drowned out by useless 
information; an interface ill suited to the current user’s presentational needs or to the content being presented; 
a system that constrains the user, rather than adapt to how it is used. UM methods often apply personalization to solve this problem.
The thought is that a system that is personal, not general, will be better at tackling information overload.

\textbf{Content discovery} is closely related to information overload. As the amount of content increases,
finding and discovering relevant pieces of information becomes more difficult. Many UM methods
exists to automatically identify and present items based on knowledge of each user. For instance,
when a system allows explicit ratings of items, individually unseen and probably relevant items can be predicted
based on past user ratings.


There are two main approaches to user modeling, each led by a different field of computer science:

\emph{Artificial Intelligence (AI)} looks at user modeling from a bottom-up, computational perspective. 
The goal is to develop algorithms that reliably and effectively can create models of each user
and through these models predict future interaction or preferences. 

\emph{Human-Computer Interaction (HCI)} looks at user modeling from a top-down, interaction based perspective.
The goal here is to create approximate models of different stereotypical users to better understand how 
an application will be used. By examining the preferences, priorities and usage patterns of different groups
of users, the application can be adapted on a personal or stereotypical level.

This thesis takes the AI approach.

Problem statement

\begin{eqnarray}
  \mathrm{M_{UM}} = (I, C, U, F, u(i_i, c_u))\\
\end{eqnarray}

\subsection{Recommender Systems}

\begin{eqnarray}
  \mathrm{M_{RS}}   &=& (I, C, U, F,    u(i_i, c_u))\\
  \mathrm{M_{RSQ}}  &=& (I, C, Q, U, F, u(q_i, i_j, c_u))
\end{eqnarray}

Based on \cite[p2]{Adomavicius2005}.


\cite[p5]{Sugiyama2004} Example of collaborative recommendation: 
"Collaborative filtering can be represented as the problem of pre- dicting missing values in a user-item ratings matrix. Figure 3 shows a simplified example of a user-item ratings matrix.
In the neighborhood-based algorithm, a subset of users is first chosen based on their similarity to the active user, and a weighted combination of their rating is then used to produce predictions for the active user."

\cite[p2]{Bell2007b}: Sparsity, variety, numbers of ratings:
"First, the numbers of users and items may be very large, as in the Netflix data, with the former likely much larger than the latter.
Second, an overwhelming portion of the user-item matrix (e.g., 99\%) may be unknown. Sometimes we refer to matrix R as a sparse matrix, although its sparsity pattern is driven by containing many unknown values rather than the common case of having many zeros.
Third, the pattern of observed data may be very nonrandom, i.e., the amount of observed data may vary by several orders of magnitude among users or among items."

\cite[p2]{Bell2007b}: Shrinkage for parameter estimation:
"Although cross validation is a great tool for tuning a few param- eters, it is inadequate for dealing with massive numbers of param- eters. Cross validation works very well for determining whether the use of a given parameter improves predictions relative to other factors. However, that all-or-nothing choice does not scale up to se- lecting or fitting many parameters at once. Sometimes the amount of data available to fit different parameters will vary widely, per- haps by orders of magnitude. The idea behind “shrinkage” is to impose a penalty for those parameters that have less data associ- ated with them. The penalty is to shrink the parameter estimate towards a null value, often zero. "



\section{Personalized Search}

An Information Retrielal Model is a quadruple \citep[p23]{Baeza-Yates1999}:

\begin{eqnarray}
  \mathrm{M_{IR}} = (D, Q, F, r(q_i, d_i))
\end{eqnarray}

$D$ is the set of representations of the documents in the system. $Q$ is the set of logical user information needs, or queries.
$F$ is a framework for modeling documents, queries and their relationships.
$R(q_i, d_i)$ is a ranking function that associates a real number with a query and a document.
Such a ranking defines the ordering among documents with regard to query $q_i$.

Similar notation for FolkRank in \cite[p4]{Hotho}.

Conceptually, this is a graph.
Nodes, and F represents the edges.

\cite[p6]{Micarelli2007}: Three dimensions of user modeling in IR: Part of retrieval; re-ranking after
retrieval; query modification based on user model.

\begin{eqnarray}
  \mathrm{M_{PS}}  &=& (D, C, Q, U, F, R)\\
  \mathrm{R}       &=& (r(q_i, d_i), u(q_i, d_i, u_i))\\
  \mathrm{P_{c_i}} &=& (Q_{c_i}, U_{c_i}, F_{c_i})
\end{eqnarray}

$P_{c_i}$ is the user model. Model based. Create heuristic-based recommendations from this info.
(Explain conten/collaborative/hybrid and orthogonal model/heuristic based approaches).

IR can be seen as content-based RS. 
If we use a collaborative RS, get a 'pseudo-hybrid' solution. Not completely integrated,
but draws on both social connections and semantic relations.

Conceptually, this is still a graph.
Nodes, and F represents the edges.

Multiple models: Better than single algorithms. Two dimensions: 
1; Different scales of data (local/regional scale).
2; Multiple source of data with tailored algorithms.

Introduce pagerank, topic sensitive pagerank, personalized topic sensitive pagerank.

Trust of sites/other users can be one algorithm in an aggregate user model.

Feedback: 
\cite[p9]{Micarelli2007} implicit from usage data or explicit user profiles. 
\cite{Shen2005}: Use of imidiate previous search history to provide real-time updating of user models. Previous searches more important.

Modeling:
\cite{Jeh2003}: Partial personalized pagerank vector, to bias pagerank towards already visited or bookmarked sites.

\cite[p2]{Teevan2005a}: Implications of personalized search. Users have different needs/domains/expectations.
"The study suggests personalization of results via re-ranking would provide significant benefit for users."
"Importantly, there are still many relevant results at ranks 11-50, well beyond what users typically see. 
This suggests the search result ordering could be improved."
Standard sets give high correlation between user relevance judgements -- however, when asking users
to rank according to their personal expectations, the disparity increases rapidly. Weakness in 
traditional IR expert relevance judgement.

Google personalized Search: Give access to store search history, personalized results thereafter. See \url{google.com/psearch} [google psearch patent]; 
Google Alerts: Emails when new results on a query is available. See \url{http://www.google.com/alerts}.

\subsection{Tags in Personalized Search}

\cite[p4]{Noll2007}: 
Tagging can be interpreted as a relation R\_tagging < D * U * T, where D is the set of documents, U the set of users and T the set of tags.

\cite[p2]{Hotho}: Folksonomies as a ranking source: 
"To this end, we propose a formal model for folksonomies, and present a new algorithm, called FolkRank, that takes into account the folksonomy structure for rank- ing search requests in folksonomy based systems."
"The FolkRank ranking scheme has been used in this paper to generate personalized rankings of the items in a folksonomy, and to recommend users, tags and resources."

\cite{Dou2007}: Evaluation of personalized search with Rank Scoring and Average rank. 
Personalization strategies: Personal click score, user interest vectors (categories), 
group-level re-ranking. MSN query logs for testing. Click-personalization simple and stable, 
profile-based more unstable.

\cite[p2]{Teevan2005a}: Implications of personalized search. Users have different needs/domains/expectations.
"The study suggests personalization of results via re-ranking would provide significant benefit for users."
"Importantly, there are still many relevant results at ranks 11-50, well beyond what users typically see. 
This suggests the search result ordering could be improved."
Standard sets give high correlation between user relevance judgements -- however, when asking users
to rank according to their personal expectations, the disparity increases rapidly. Weakness in 
traditional IR expert relevance judgement.




\section{Aggregate Modeling}

Use simple algorithms over complex, monolithic ones. E.g., one algorithm can measure freshness - 0 when the rating is out of date, and 1 when it's completely fresh.


