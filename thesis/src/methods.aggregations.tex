\section{Stacked Prediction Aggregation}

Adaptive prediction aggregation means combining the results
from multiple scalar predictors based on the current user and item
(see Section \ref{sec:theory:predictionagg}).
As mentioned, we have two levels of predictors:
The first level is a set of traditional recommender systems
that produce estimations of unknown ratings between users and items.
The second level is another set of recommender systems 
that predict how accurate each of the first level recommenders will be.

In this section, we shall explain how such a system may be created.
Most importantly, there are two distinct phases to stacked user modeling:

\begin{enumerate*}
  \item The modeling phase creates the user models for both levels.
  \item The prediction phase uses the created models to estimate ratings.
\end{enumerate*}

We shall first explain the modeling phase, then the prediction phase,
when dealing with prediction aggregation.
The next section will explain a similar situation where
we wish to do \emph{adaptive rank aggregation}: 
combining ordered lists of results, depending on the current user and item.


\subsection{Modeling Phase}

Listing \ref{code:training} gives the basic algorithm for training
our models. The input to this method is the standard ratings matrix,
and a set of untrained modeling methods (in this case,
untrained recommender systems).

\begin{algorithm}
  \begin{algorithmic}[1]
  \REQUIRE ratings: The ratings matrix
  \REQUIRE methods: The set of modeling methods
  \ENSURE
    \STATE $rating\_models \gets \emptyset$
    \STATE $error\_models \gets \emptyset$
    \FORALL{$m \in methods$}
      \STATE $sample \gets \mathrm{BootstrapSample}(ratings)$
      \STATE $rating\_models_m \gets \mathrm{TrainModel}(m, sample)$
      \STATE $error\_models_m  \gets \mathrm{TrainErrorModel}(rating\_models_m, ratings)$
    \ENDFOR 
  \RETURN $(rating\_models, error\_models)$
  \end{algorithmic}
  \caption[Adaptive Prediction Aggregation Modeling]{Adaptive Prediction Aggregation Modeling
  }
  \label{code:training}
\end{algorithm}

An important question is how we should split the ratings data.
In this scenario, we need to split the data for a number of purposes.
The following sets must be created during training:

\begin{enumerate*}
  \item Training sets to create the standard recommenders.
  \item Training sets to create the error estimation models.
  \item A testing set to test our final system.
\end{enumerate*}

Constructing these subsets of the available data is a common task in ensemble learning
\cite[p7]{Polikar2006}.
As seen in Listing \ref{code:training}, we use an approach called 
\emph{bootstrap aggregation}, also known as \emph{bagging}
(introduced by \cite{Breiman1996}).
Originally, bagging is used by ensemble learning classification methods, where multiple classifiers are 
trained by uniformly sampling a subset of the available training data. 
Each model is then trained on one of these subsets, and the models are aggregated by averaging their individual predictions.

Formally, given a training set $D$ with $n$ items, bagging creates $m$ new training sets of size $n' \leq n$ by sampling
items from $D$ uniformly and with replacement. 
In statistics, these types of samples are called \emph{bootstrap samples}.
If $n'$ is comparable in size to $n$, there will be some items
that are repeated in the new training sets.

Bagging suits our needs perfectly, for a few reasons: First, the method helps create disjoint predictors, 
since each predictor is only trained (or specialized for) a subset of the available data.
Second, it allows us to easily train the underlying modeling methods without any complex partitioning of the data.
Our partitioning strategy is now clear:

\begin{enumerate*}
  \item Split the entire dataset into a training and testing set.
  \item Train modeling methods through bootstrap aggregation of the training set.
  \item Train error models from the complete training set.
  \item Test the resulting system with the initial testing set.
\end{enumerate*}

Each modeling method is trained in ways specific to their implementation. 
Model-based approaches create pre-built strutures and provide offline training,
while heuristic methods simply store the data for future computation.
Either way, it is up to each modeling method what it does with the supplied training data.
The result of this algorithm is a set of trained rating models and error models.

\begin{algorithm}
  \begin{algorithmic}[1]
  \REQUIRE ratings: The ratings matrix
  \REQUIRE rating\_model: A standard user model
  \ENSURE
    \STATE $errors \gets [[]]$
    \FORALL{$user,item,rating \in ratings$}
        \STATE $errors_{user,item} \gets | ratings_{user,item} - \mathrm{Predict}(rating\_model, user, item) |$
    \ENDFOR 
    \STATE $error\_method \gets \mathrm{NewModelingMethod}(SVD)$
    \STATE $error\_model  \gets \mathrm{TrainModel}(error\_method, errors)$
  \RETURN $error\_model$
  \end{algorithmic}
  \caption[Prediction Error Modeling]{Prediction Error Modeling}
  \label{code:trainerrormodel}
\end{algorithm}

Listing \ref{code:trainerrormodel} shows an algorithm for training the error models.
The input is the entire ratings matrix, and a trained recommender model
that this error model should represent.
We first create the aforementioned error matrix by estimating
predictions for each known combination in the ratings data.
The $\mathrm{NewModelingMethod}$ call simply creates a new, untrained
recommender model of some prespecified $type$
(in this case, a new SVD-based model, but any recommender method will do).
A new model is then trained based on the created error matrix,
and returned as our new $error\_model$.

When the computations of the algorithm in Listing \ref{code:training} is complete,
we have a set of trained recommender systems, and a set of trained error models.
Each recommender model has a corresponding error model,
forming a set of stacks, that we shall use when performing predictions.


\subsection{Prediction Phase}

In the prediction phase of adaptive prediction aggregation,
we wish to use our stacks of trained models to produce adaptive
combinations of multiple predictions and accuracy estimations.
Listing \ref{code:prediction} gives the basic algorithm.

\begin{algorithm}
  \begin{algorithmic}[1]
  \REQUIRE user, item: A user and an item
  \REQUIRE rating\_models: The set of trained modeling methods 
  \REQUIRE error\_models: The set of trained error models
  \ENSURE
    \STATE $ratings \gets \emptyset$
    \STATE $errors  \gets \emptyset$
    \FORALL{$m \in rating\_models$}
      \STATE $ratings \gets \mathrm{Predict}(rating\_models_m, user, item)$
      \STATE $errors  \gets \mathrm{Predict}(error\_models_m, user, item)$
    \ENDFOR 
    \STATE $errors \gets \mathrm{Normalize}(errors)$
    \STATE $prediction \gets 0$
    \FORALL{$m \in rating\_models$}
      \STATE $weight_m \gets 1 - error_m$
      \STATE $prediction \gets prediction + weight_m \cdot ratings_m$
    \ENDFOR
 
  \RETURN $prediction$
  \end{algorithmic}
  \caption[Adaptive Prediction Aggregation]{Adaptive Prediction Aggregation
  }
  \label{code:prediction}
\end{algorithm}

The first input is the user and item for which we wish to predict a rating.
We assume that this rating is unknown --- predicting ratings for known combinations
would mean recommending items the user has already seen and considered
(however, if we are dealing with a task such as personalized search,
these known ratings are of course important, as we shall see in the next section).

The other inputs are the trained rating models, and the corresponding error models.
The algorithm begins by creating empty sets for predicted ratings and errors.
Next, each modeling method is used to predict ratings, and their error models to predict errors.
Note that each step in the first for-loop is independent of each other, and both steps
inside the for loop is also independent. This is then an algorithm well suited for
parallelization.
In a MapReduce framework, this for loop would be run as a $\mathrm{map}$ operation,
where the input user and item is mapped over the sets of modeling methods
(we will get back to this in Section \ref{sec:implementation}).

After the predictions have been collected, the errors are normalized,
i.e. converted to the range $[0,1]$ and changed to sum to $1$.
This is vital before last stage of the prediction algorithm,
which weighs each prediction from the different rating models.
This step corresponds to the previously explained $\mathrm{reduce}$ operation,
that combines multiple scores into one final result.
The weight of each method is computed as $1 - error$, where $error$ 
is the normalized error for this method, for the current user and item.
Each rating prediction is then weighted, and combined to form the final,
adaptively aggregated prediction.

There is an important performance different between the modeling and prediction phases:
While the modeling phase is the most computationally expensive,
it can be performed independently of making predictions.
As the prediction phase is when the user has to wait for the system,
this is of course where performance is most important.
Naturally, as users rate more items and new items arrive,
the models have to be recreated based on this new reality.
However, as the modeling phase is an offline operation,
the training can be performed in the background, while new
and speedy predictions are always at the users hands.


\section{Stacked Rank Aggregation}
\label{sec:methods:rank}

Now that we have seen how to adaptively aggregate scalar scores, it is time to see how
to do \emph{adaptive rank aggregation}. Rank aggregation means combining sorted lists of items.
In this scenario, each modeling method takes the current user as input, and produces a
list of items ranked in order of rating
(see Section \ref{sec:theory:rank}).

Aggregating lists is desirable in a number of situations:
Often we wish to produce lists of recommended items, not just estimate the rating of a single combination.
Consider the task of personalizing a list of search results
(see Section \ref{sec:search}). The important part is not the score
given to each result, but rather the order in which they appear.
The underlying technology is still the same: a number of recommenders are used to predict the ratings
of items to users. However, to do rank aggregtion, another layer is added, that requests lists from each method,
not only singular items.

Because it is such an important use case, we shall use personalized search to present our approach to adaptive rank aggregation.
In addition to the standard recommenders, we have an information retrieval method,
as introduced in Section \ref{sec:ir}.
The IR method takes in a user-initiated query (a collection of words or a sentence), and returns a number of 
search results, in an ordered list.
In traditional personalized search, a recommender system can then be used to estimate a rating for each of the returned items,
and re-sort, or re-score, the results list (e.g. \citet[p3]{Xu2008}).

The key insight is that both the IR method and the recommender systems form \emph{input signals}
(see Section \ref{subsec:signals}).
An input signal is some measure of how each item should be ranked in the final results list.
The relevance scores returned from our IR ranking functions are signals,
and the predicted ratings from each recommender systems are signals.
Adaptive aggregation then entails estimating \emph{how accurate ach of these signals are likely to be for the current user and item}.
This is almost the same task as in adaptive prediction aggregation, only in a list-oriented fashion.

The important difference is this: The IR methods constrict the range of items worked on by the recommender systems.
As the IR methods identify items that may be relevant to the users query, these are the items we wish the recommender systems to work on.
This goes back to the previously mentioned difference between \emph{search} and \emph{recommendations}:

\begin{itemize*}
  \item Recommenders find relevant items the user does not already know exists.
  \item Search engines find relevant items the user knows or hopes exists.
\end{itemize*}

The difference lies in the knowledge of existence.
As personalized search is still a search task, the IR methods should determine the set of items that might be relevant.
Their relevance scores for these items becomes the first input signals.
The recommender systems works on this set of items, rescoring each as needed.
We still have the adaptive layer that estimates how well each signal will perform for the current user and item.
This is especially important considering that we may have multiple IR methods that define multiple sets of relevant items.
The final result is an adaptive combination of the rating and accuracy preictions for each signal,
as seen in Figure \ref{fig:adaptiverank}.
Let us now see how the modeling and prediction phases are performed in adaptive rank aggregation.

\input{figure.rank}

\subsection{Modeling Phase}

We shall only deal with settings where we have a single IR method.
While multiple IR methods and corresponding error models would be an interesting
setting, we are most interested in using the IR method for constraining the Item-space considered by the recommender systems.
As we shall see, this does not introduce many changes to our previously developed algorithms.

The modeling phase for the recommender system stays the same, with one important change.
As we are dealing with a search engine, we might not have an explicit ratings matrix to rely on.
Most feedback we can gather from user initiated searches are from query logs.
These logs show the current user, query, and the item that is finally selected after the query is performed.
Query log mining is a common approach in personalized search
(e.g. \cite{Liu2002, Sugiyama2004, Shen2005, Speretta2000}).
By mining this log, we can create an implicit ratings matrix.
Each populated cell represents a selected item.

The values in this implict ratings matrix can take many forms.
If we only care about selected items, binary ratings may suffice:
selected items are then represented by a $1$ in the ratings matrix.
These ratings can be further improved by considering different metrics, including:

\begin{itemize*}
  \item Time spent before selecting the item.
  \item How far the user was willing to scroll before clicking the item.
  \item Whether or not the user resubmitted the same query shortly after.
\end{itemize*}

Based on these, and similar, metrics, we can achieve quite accurate implicit ratings.
Naturally, ratings can also be gathered from other sources.
If we have more data on each user, or know of secondary systems such as social networks
or other systems where ratings are present, these can be used to augment the implicit ratings matrix.
There are also search systems where we already have explicit ratings:
Consider, for instance, the use case of searching for movies on a movie rating site,
or searching for people in a social network.
In these cases, we have explicit ratings that can be used to train the recommender models.

When we have the implicit or explicit ratings matrix, the modeling phase
consists of two parts: training the IR models and the recommender models.
The recommender models are trained as before, given in Listing \ref{code:training}.
The one or more IR methods are not trained with a ratings matrix,
but with the items and their respective data.
Of course, the actual IR modeling method depends on the IR system itself,
which is not our concern in this chapter.


\subsection{Prediction Phase}

\begin{algorithm}[t]
  \begin{algorithmic}[1]
  \REQUIRE user: The current user
  \REQUIRE items: The set of all items and their meta-data
  \REQUIRE query: The user initiated query
  \REQUIRE ir\_model: A trained IR model
  \REQUIRE rating\_models: The set of trained modeling methods 
  \REQUIRE error\_models: The set of trained error models
  \ENSURE
    \STATE $ratings \gets \emptyset$
    \STATE $errors  \gets \emptyset$
    \STATE $results \gets \mathrm{Search}(ir\_model, items, query)$
    
    \FORALL{$item \in results$}
      \FORALL{$m \in rating\_models$}
        \STATE $ratings_{m,item} \gets \mathrm{Predict}(rating\_models_m, user, item)$
        \STATE $errors_{m,item}  \gets \mathrm{Predict}(error\_models_m, user, item)$
      \ENDFOR 
    \ENDFOR
    \STATE $errors \gets \mathrm{Normalize}(errors)$

    \FORALL{$item, ir\_score \in results$}
      \STATE $prediction \gets ir\_score$
      \FORALL{$m \in rating\_models$}
        \STATE $weight_{item} \gets 1 - error_{m,item}$
        \STATE $prediction \gets prediction + weight_m \cdot ratings_{m,item}$
      \ENDFOR
      \STATE $item_{prediction} \gets prediction$
    \ENDFOR
    
    \STATE $results \gets \mathrm{SortByPredictions}(results)$
  \RETURN $results$

  \end{algorithmic}
  \caption[Adaptive Rank Aggregation]{Adaptive Rank Aggregation}
  \label{code:rank:prediction}
\end{algorithm}

The prediction phase is where adaptive rank aggregation differs most from adaptive prediction aggregation.
Listing \ref{code:rank:prediction} gives the basic algorithm.
As input, instead of one item, we have the entire set of items, and a query.
We run the query and items through the IR model to get the constrained set of items ($results$).
Each of the recommender methods is then run for each of the items in the results list.
As before, the first for-loop can easily be peformed in parallel.
Each call to $\mathrm{Predict}$ is independent of the other operations,
allowing us to perform it as a $\mathrm{map}$ operation.

As before, the error estimations are normalized before converting them to weights.
Since we are dealing with two dimensions of errors, for each item and each method,
the errors are normalized across items.
In other words, for each item, the errors from the recommenders fall in the range $[0,1]$ and sum to $1$.

After each item in the results list has an IR score, a set of predictions, and a corresponding set of 
error predictions, the adaptive aggregated prediction is computed. 
Because we do not care of the final score we set the initial predictions to be the IR scores.
The recommender systems simply add or substract from this initial score.
This means that the resulting predictions will not be in the same range as the known ratings,
but since we are only interested in the order of the items, not the actual rating, this poses no problem.

After computing the predictions for each item in the results list, 
we sort the list by the item predictions, and return the list.
The resulting list is adaptively sorted based on the current user and the specific items in the list,
achieving in adaptive rank aggregation.

Clearly, as in prediction aggregation, the strength of our resulting system is in large part dependent on the accuracy of our ratings.
This means that deciding and understanding how implicit ratings are created, or 
finding auxiliary sources to provide explicit ratings, is a critical step.
As we have said before, algorithms are only as strong as the data they can leverage.
In other words, methods for personalized search will work best in settings where we have explicit ratings,
or can gather explicit ratings from secondary sources, for example from external social networks or publishing platforms.


