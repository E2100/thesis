\section{Recommenders}

We need a number of recommender systems for each experiment.
Standard recommenders will be used for both the basic predictions,
and the accuracy estimations,
as desecribed in Chapter \ref{chap:methods}.

Naturally, we need a large number of different recommenders, preferably ones that consider
disjoint patterns in the data. Table \ref{table:results:methods}
gives a short overview of the recommender systems we shall employ.
Each experiment will use every recommender in this table.
This section only gives a short introduction to each recommender.
See Section \ref{sec:recommender} for more information, 
and Appendix \ref{appendix:implementation} for details on how these were implemented in our system.


\subsection{Basic Recommenders}

\begin{table}[t]
  \begin{tabular*}{\textwidth}{ l l l l }
    \toprule
    ~Â & \emph{method} &  \emph{algorithm} & \emph{description} \\
    \midrule
    S & svd1          & SVD                   & ALSWR factorizer, 10 features. \\
    S & svd2          & SVD                   & ALSWR factorizer, 20 features. \\
    S & svd3          & SVD                   & EM factorizer, 10 features. \\
    S & svd4          & SVD                   & EM factorizer, 20 features. \\
    S & slope\_one    & Slope One             & Rating delta computations. \\
    S & item\_avg     & Baseline              & Based on item averages. \\ 
    S & baseline      & Baseline              & Basd on user and item averages.\\ 
    S & cosine   	    & Cosine similarity     & Weigted ratings from similar items.\\ 
    S & knn       	  & Pearson Corr.         & Weighted ratings from similar users.\\
    \midrule
    A & median    	  & Aggregation           & Median rating from the above methods. \\
    A & average    	  & Aggregation           & Average rating from the above methods. \\
    A & stacked       & Adaptive agg.         & Accuracy predictions from error models. \\
    \bottomrule
  \end{tabular*}
  \caption[Stacked Modeling Methods]{
    Stacked modeling methods: A short overview of each of the recommender methods
    used in our experiment.
    Each recommender is used in every experiment. 
    See Section \ref{sec:recommender} for more information.
  }
  \label{table:results:methods}
\end{table}

As seen in Table \ref{table:results:methods}, we have two types of recommenders:
First, we have the basic recommenders, denoted by \emph{S} in the table.
These recommenders each look at the data in different ways to arrive at predicted ratings.
We chose this wide range of recommenders for just this reason:
as previously explained, the performance of aggregate recommenders
are more dependent on the disimilarity of the basic recommenders
than their individual performance.

Let us briefly explain how each basic recommender works.
The SVD methods look for global patterns in the data 
by reducing the ratings-space into a concept-space.
By reducing this space, the algorithm is able to find
latent relations, such as groups of movies that has the same
rating pattern, or groups of users that often rate in a similar manner.

For the SVD methods, the factorizers refers to algoriithms used to factorize the ratings matrix.
An EM factorizer uses the Expectation-Maximization algorithm to find the factors.
An ALSWR factorizer performs the same factorization with a least-squares approach \citep{Zhou2008}.
The number of features refers to the truncation of the factors in order to reduce the taste-space.
In other words, the number of features corresponds to the number of latent taste categories
we wish to identify.
Naturally, different numbers of features will yield different recommenders.

The Slope One and baseline algorithms look at average
ratings for items and from users, and use these to predict ratings.
These are simple algorithms that often perform as well
as more complex approaches.

The cosine similarity algorithm looks for items that are rated
similarly by the same users, and infers item similarity from this measure.
New ratings are then predicted by taking known ratings of other items,
weighted by their item's similarity to the new item.

The KNN algorithm employs yet another approach. This algorithm,
similar in strategy to the cosine similarity algorithm,
looks for users with similar rating patterns.
The similarity is measured with the Pearson Correlation Coefficient.
Predictions are created by collecting ratings from similar users
of the item in question, weighted by their respective similarity.
See Section \ref{sec:recommender} for more 
detailed information on how these recommenders work. 

The main difference between our recommenders are the scope of the patterns they leverage.
The SVD and baseline methods look at global effects, such as latent categories
and overall rating averages.
The cosine similarity and KNN algorithms look at smaller clusters of similar
users and items, and compute an average rating weighted by 
similarities of elements.
This wide range of recommender should give us the desired
effect of looking at disjoint data patterns.


\subsection{Stacked Recommenders}

The second type of recommenders are the aggregation methods, 
that combine the result of each of the basic recommender systems
(the methods below the middle line in Table \ref{table:results:methods},
denoted "A").

The first two of these methods are simple aggregation approaches.
These will be used to test hypothesis H2.
The median aggregator choses the median value of the predictions
produced by the standard recommenders.
Similarly, the average aggregator takes the mean of the
standard predictions.
While not complex in nature, these methods
will help us see how our method compares to simple, traditional
aggregation techniques.

The last entry in Table \ref{table:results:methods}
refers to our technique. 
This is the recommender outlined by the algorithms
in Chapter \ref{chap:methods},
that create secondary accuracy estimating recommender systems,
in order to adaptively weigh each basic recommender.
All the aggregation approaches, including our technique,
use every basic recommender system described so far.

As explained in Section \ref{sec:usermetamodeling},
any basic recommender system can be used for the stacked method.
The only difference is how this method is trained:
while the basic methods are trained using the ratings matrix,
the stacked methods are trained using the error model,
as seen in Listing \ref{code:training}.
In other words, we have as many possibilities for choosing
the stacked recommenders as the basic recommenders.

For our experiment, we went with SVD recommenders
for each of the stacked models.
That is, each basic recommender method gets a secondary 
accuracy predicting recommender, which in this case is a 
standard SVD recommender.
The SVD recommender is a natural choice in this case,
since we wish to uncover latent patterns of accuracy
for each model.
Examples of these patterns include groups of items
or users a specific recommender works well for.

The choices of recommenders will be further discussed
in Chapter \ref{chap:discussion}.
For now, let us see how the performance of each method is determined.


