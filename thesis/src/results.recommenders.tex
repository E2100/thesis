\section{Recommenders}

We need a number of recommender systems for each experiment.
Standard recommenders will be used for both the basic predictions,
and the accuracy estimations,
as described in Chapter \ref{chap:methods}.

Naturally, we need a large number of different recommenders, preferably ones that consider
disjoint patterns in the data. Table \ref{table:results:methods}
gives a short overview of the recommender systems we shall employ.
Each experiment will use every recommender in this table.
This section only gives a short introduction to each method.
See Section \ref{sec:recommender} for more information, 
and Appendix \ref{appendix:implementation} for details on how these were implemented in our system.


\subsection{Basic Recommenders}

\begin{table}[t]
  \begin{tabular*}{\textwidth}{ l l l l }
    \toprule
    ~Â & \emph{method} &  \emph{algorithm} & \emph{description} \\
    \midrule
    S & svd1          & SVD                   & ALSWR factorizer, 10 features. \\
    S & svd2          & SVD                   & ALSWR factorizer, 20 features. \\
    S & svd3          & SVD                   & EM factorizer, 10 features. \\
    S & svd4          & SVD                   & EM factorizer, 20 features. \\
    S & slope\_one    & Slope One             & Rating delta computations. \\
    S & item\_avg     & Baseline              & Based on item averages. \\ 
    S & baseline      & Baseline              & Based on user and item averages.\\ 
    S & cosine   	    & Cosine similarity     & Weighted ratings from similar items.\\ 
    S & knn       	  & Pearson Corr.         & Weighted ratings from similar users.\\
    \midrule
    A & median    	  & Aggregation           & Median rating from the above methods. \\
    A & average    	  & Aggregation           & Average rating from the above methods. \\
    A & adaptive      & Adaptive agg.         & Accuracy predictions from error models. \\
    \bottomrule
  \end{tabular*}
  \caption[adaptive Modeling Methods]{
    adaptive modeling methods: A short overview of each of the recommender methods
    used in our experiment.
    Each recommender is used in every experiment. 
    See Section \ref{sec:recommender} for more information.
  }
  \label{table:results:methods}
\end{table}

As seen in Table \ref{table:results:methods}, we have two types of recommenders.
First, we have the basic recommenders, denoted by \emph{S} in the table.
Each system looks at varying data patterns to predict ratings.
We chose this wide range of recommenders for just this reason:
as previously explained, the performance of aggregation methods
are more dependent on the dissimilarity of the basic recommenders
than their individual performance.

Let us briefly explain how each basic recommender works.
The SVD methods look for global patterns in the data 
by reducing the ratings-space into a concept-space.
By reducing this space, the algorithm is able to find
latent relations, such as groups of movies that has the same
rating pattern, or groups of users that often rate in a similar manner.

For the SVD methods, the factorizers refers to algorithms used to factorize the ratings matrix
(see Section \ref{subsec:recommender:examples}).
The Slope One and baseline algorithms look at average
ratings for items and from users, and use these to predict ratings.
These are simple algorithms that often perform as well
as more complex approaches.

The cosine similarity algorithm looks for items that are rated
similarly by the same users, and infers item similarity from this measure.
New ratings are then predicted by taking known ratings of other items,
weighted by their item's similarity to the new item.
This is based on the same method used in the previously introduced vector space model.

The KNN algorithm employs yet another approach. This algorithm,
similar in strategy to the cosine similarity algorithm,
looks for users with similar rating patterns.
The similarity is measured with the Pearson Correlation Coefficient.
Predictions are created by collecting ratings from similar users
of the item in question, weighted by their respective similarity.
See Section \ref{sec:recommender} for more 
detailed information on how these recommenders work. 

The main difference between our recommenders are the scope of the patterns they leverage.
The SVD and baseline methods look at global effects, such as latent categories
and overall rating averages.
The cosine similarity and KNN algorithms look at smaller clusters of similar
users and items, and compute an average rating weighted by 
similarities of elements.
This wide range of recommender should give us the desired
effect of looking at disjoint data patterns.


\subsection{Adaptive Recommenders}

The second type of recommenders are the aggregation methods, 
that combine the result of each of the basic system
(the methods below the middle line in Table \ref{table:results:methods},
denoted ``A'').

The first two of these methods are simple aggregation approaches.
These will be used to test hypothesis H2.
The median aggregation method choses the median value of the predictions
produced by the standard recommenders.
Similarly, the average aggregation method takes the mean of the
standard predictions.
While not complex in nature, these methods
will help us see how our method compares to simple, traditional
aggregation techniques.

The last entry in Table \ref{table:results:methods}
refers to our technique. 
This is the recommender outlined by the algorithms
in Chapter \ref{chap:methods},
that create secondary accuracy estimating recommender systems,
in order to adaptively weigh each basic recommender.
All the aggregation approaches, including our technique,
employ every basic recommender system described so far.

As explained in Section \ref{sec:usermetamodeling},
any basic recommender system can be used for the adaptive method.
The only difference is how this method is trained:
while the basic methods are trained using the ratings matrix,
the adaptive methods are trained using the error model,
as seen in Listing \ref{code:training}.
In other words, we have as many possibilities for choosing
the adaptive recommenders as the basic recommenders.

For our experiment, we went with SVD recommenders
for each of the adaptive models.
That is, each basic recommender method gets a secondary 
accuracy predicting recommender, which in this case is a 
standard SVD recommender.
The SVD recommender is a natural choice in this case,
since we wish to uncover latent patterns of accuracy
for each model.
Examples of these patterns include groups of items
or users a specific recommender works well for.

It is important to note that the same configuration of recommenders was used for all three experiments.
In other words, neither the basic nor the aggregate or adaptive recommenders were heavily tailored
to each dataset. To be sure, higher performance could probably have been achieved
by tailoring each recommender to each dataset. 
However, as our goal is to compare our finite set of methods, 
we are currently only interested in how they perform compared to each other.

As with the basic recommenders, the same SVD recommender configuration was used 
for the adaptive layer in each Experiment.
We chose to use en EM-factorizer to perform the actual decomposition,
consisting of 20 features. The decomposition was performed by 20 iterations.
See \ref{subsec:recommender:examples} for more information on how SVD recommenders work. 
The choices of recommenders will be further discussed
in Chapter \ref{chap:discussion}.

