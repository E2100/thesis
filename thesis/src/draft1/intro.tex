\textbf{Glossary}

\begin{itemize*}
  \item IR: Information Retrieval 
  \item RS: Recommender System
  \item PS: Personalized Search
  \item D:  Set of Documents
  \item Q:  Set of Queries  
  \item C:  Set of Users
  \item U:  Set of Utilities/Ratings
  \item F:  Set of Relations
  \item M:  A Model of Computation
  \item u,v: User indices
  \item i,j: Document indices
  \item SERP: Search Engine Results Page
\end{itemize*}

\textbf{Introduction to Search, Information Retrieval}

\begin{itemize*}
  \item The history of search, from libraries, to the web to massive datasets
  \item IR is all about finding documents
  \item Deals with information overload \cite{Bjorkoy2010d}
  \item User knows what he wants, but not where it is or if it exists
  \item Problem: Lack of data, increasing amounts of unreliable info, no personalization
\end{itemize*}

"Information retrieval (IR) is finding material (usually documents)
of an unstructured nature (usually text) that satisfies an
information need, from within large collections 
(usually stored on computers)" - \cite[p1]{Manning2008}.

\cite[p2]{Micarelli2007}: Three information access paradigms: browsing, searching by query, recommendations.

Problems: Polysemy, synonymy \cite[p4]{Micarelli2007}.

\cite[p6]{Bender2005}: The Free Web:
"Another big difference between the web and traditional well controlled collections is that there is virtually no control over what people can put on the web. Couple this flexibility to publish anything with the enormous influence of search engines to route traffic and companies which deliberately manipulating search engines for profit become a serious problem. This problem that has not been addressed in traditional closed information retrieval systems."

\cite[p1]{Silverstein1999}: Web search differ from traditional IR:
"Our data supports the conjecture that web users differ significantly from the user assumed in the standard information retrieval literature. 
Specifically, we show that web users type in short queries, mostly look at the first 10 results only, and seldom modify the query. 
This suggests that traditional informa- tion retrieval techniques may not work well for answering web search requests. 
The correlation analysis showed that the most highly correlated items are constituents of phrases. 
This result indicates it may be useful for search engines to consider search terms as parts of phrases even if the user did not explicitly specify them as such."

\cite[p1]{Lee2005}: Navigational vs Informational queries: "The goal of a user can be classified into at least two categories: navigational and informational.
A query is considered navigational when a user has a particular Web page in mind and is primarily in- terested in visiting the page. 
Informational queries, on the other hand, refer to the queries where the user does not have a particular page in mind 
or intends to visit multiple pages to learn about a topic."

\cite[p12]{Bender2005}: Aggregate search hit rankings. Already from earliest Google:
"The ranking function has many parameters like the type-weights and the type-prox-weights. Figuring out the right values for these parameters is something of a black art. In order to do this, we have a user feedback mechanism in the search engine. A trusted user may optionally evaluate all of the results that are returned."

\cite[p1]{Chirita2010}: Query expansion:
"Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly. "

\cite[p1]{Beitzel2004}: Queries varies houerly:
"The queries received during peak hours are more similar to each other than their non-peak hour counterparts."
"Some topical categories vary substantially more in popularity than others as we move through an average day."

\textbf{Introduction to User Modeling, Recommender systems}

\begin{itemize*}
  \item History of recommendations, from libraries, to the web
  \item UM about tailoring application to its users
  \item RS is all about recommending new items
  \item Also deals with information overload
  \item Recommendations in two categories: recommended and similar items
  \item Unlike in search, the user does not know what he wants
  \item Problem: One method is not enough for complex recommendations. People are different. 
\end{itemize*}

Problems: Privacy \cite[p3]{Micarelli2007}.

\textbf{About integrating, using many algorithms, robust models}

IR and RS/UM, two sides of same coin.
RS is IR with an active item as basis for similiarity.
IR is RS with the query as the active item as basis for recommendations/similarity.
Combining should be a trivial, common task.

When two separate modules: Not a hybid system. Content + collaborative based.
[Develop models for both non-hybrid and hybrid algorithms!]

Misguided notion that RS is for problem A and IR for problem B.
Two fields that should help each other live in complete disconnect.
This paper: Bridge the gap, find common ground.


\cite{Teevan2007}: Repeat queries, navigational queries:
"People often repeat Web searches, both to find new information on topics they have previously explored and to re-find information they have seen in the past.
 The query associated with a repeat search may differ from the initial query but can nonetheless lead to clicks on the same results. 
This paper explores repeat search behavior through the analysis of a one-year Web query log of 114 anonymous users and a separate controlled survey of an 
additional 119 volunteers. Our study demonstrates that as many as 40\% of all queries are re-finding queries.
Re-finding appears to be an important behavior for search engines to explicitly support, and we explore how this can be done. 
We demonstrate that changes to search engine results can hinder re-finding, and provide a way to automatically detect repeat searches and predict repeat clicks."

\cite[p12]{Bell2007}: Combining different recommender systems:
Our final solution (RMSE=0.8712) consists of blending 107 individual results. Since many of these results are close variants, 
we first describe the main approaches behind them. Then, we will move to describing each individual result.

\cite[p75]{Bell2007d}: Combining different recommender systems 2:
"We found that it was critically important to utilize a variety of methods because the two main
tools for collaborative filtering address quite different levels of structure in the data. (Much more!)

\cite{Nielsen1998}: 
Web personalization is much over-rated and mainly used as a poor excuse for not designing a navigable website
Personalization does work in a few, limited cases that are characterized by being: very simple to describe in machine-understandable ways, and
relatively unchanging". Do not impose any work on part of the user!
Important in 10 years, not now -- been 10 years!

\begin{itemize*}
  \item Because of the separate problems, little work on itegration
  \item Social search often a different meaning -- search social networks
  \item Why they complement each other
  \item More data beats better algorithms -- users are more data, 
        a fact often neglected by traditional IR
  \item Combining many algorithms beats one single algorithm, e.g. Netflix.
  \item More algs equals more data!
\end{itemize*}

\textbf{Summary of problem statement, motivation, main hypothesis}

Main hypothesis:

\begin{itemize*}
  \item $H_0$: A User Model can be created through a combination of different personalization algorithms to provide personalized IR.
  \item $H_2$: The personalized search performed by such models outperforms single algorithms and pure IR in the Rank Scoring and Average Rank metrics.
\end{itemize*}

\textbf{Contributions}

\begin{itemize*}
  \item Framework for providing IR with UM/personalization
  \item More data for IR methods to leverage
  \item System for creating aggregate user models
\end{itemize*}

\textbf{Thesis outline}

Outline.


