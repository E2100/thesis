\section{Rank Aggregation}

\input{table.rank.washington}
\afterpage{\clearpage}

Let us see how adaptive recommenders can be used for personalized search.
While the previous experiment was a quantitative exploration of RMSE values,
this experiment will focus on qualitative traits of rank aggregation.

The MovieLens dataset fit the needs of this experiment.
Searching through movies is a scenario where the actual predicted
rating of each movie would sometimes be a welcome signal for ranking the results.
We have a database of movies the user wishes to search through,
where the results are ranked both by how well they match the free text query,
and according to the predicted rating of each movie for the current user.
The value of letting predicted ratings re-rank search results of movies
is entirely dependent on the actual system and use case,
but it presents a probable situation where personalized search may be employed.

Hypothesis H3 states that 
{
  \itshape
  the ordering of results from an information retrieval query
  can be personalized by using adaptive recommenders.
}
We wish to check if the prediction algorithm
in Listing \ref{code:rank:prediction} performs personalized search
in a meaningful way.
There are a few important limitations to this experiment:

\begin{itemize}
  \item 
    We are not interested in measuring the actual performance of the IR system.
    It us assumed that the IR model returns items relevant to the current query,
    ranked by their individual relevance.
  \item
    We are not interested in measuring the performance of the resulting personalized search.
    This experiment will only show whether or not personalized search is achievable
    by using adaptive recommenders, as per our hypothesis.
\end{itemize}

\begin{comment}
\begin{table}[b]
  \centering
  \begin{tabular*}{0.7\textwidth}{ l l l l }
    \toprule
      ~Â & 
      \emph{query} &
      \emph{scores} &
      \emph{IR weight} \\
    \midrule
    
    1 &
    [``new york or washington''] &
    combined &
    $1.0$ \\

    2 &
    [star trek] &
    combined &
    $0.3$ \\

    3 &
    [paris] &
    ratings &
    $0.0$ \\

    4 &
    [1998] &
    ratings &
    $0.0$ \\

    \bottomrule 
  \end{tabular*}
  \caption[List of Ranking Experiments]{List of ranking tests in this section.}
  \label{table:experiments:rank}
\end{table}
\end{comment}

This experiment is not statistically significant in any way,
but rather a case study where we investigate the implications
of using adaptive recommenders for personalized search.

To be sure, there are many ways of determining the accuracy of a personalized search
algorithm. Examples include the mean average precision of the results list
(the mean of the precision average over a set of queries).
As always when dealing with personalization, 
these are subjective measures based on relevance judgements from each user.

Our hypothesis only states that our algorithm should be usable for such 
a system, which is what we shall explore in this section.
To quantitatively measure the performance of personalized search,
one would need detailed query logs, user profiles and click-through information.

Any recommender system can be used for personalized search.
The interesting bit in regards to adaptive recommenders is what 
happens under the hood. First, the information retrieval score 
is itself treated as an input signal, just as the user modeling methods.
Second, by using adaptive recommenders, the user is in control of which
methods that actually determine how the final results are ranked.

We have considered four use cases with to see how our algorithm
performs in a number of scenarios. Each case presents 
a query and shows how a certain IR weight influences the final ranking.

The IR weight is the scalar value multiplied with the IR score 
before the adaptive recommender scores are incorporated in the result
(see Listing \ref{code:rank:prediction}).
The actual choice of weight depends on scale of scores
returned by the IR method, and how much the IR model should influence the final ranking.
If the scores are on the same scales as the ratings themselves,
an IR weight of $1$ signifies that the IR score
should have equal importance as each recommender score.
Any higher, and the IR model should be prioritized above the recommenders.
Any lower, and the recommender scores will dominate the initial IR rankings.

The actual IR weight must
be calculated based on the scale of the IR scores.
In this chapter, the scores returned by our IR
model is normalized to the scale of our ratings.
We then adapt the IR weight to achieve differing prioritisations of the IR scores.

We consider the following use cases:
(1) searching for multiple topics,
(2) searching for a series of movies,
(3) searching for one particular topic, and
(4) searching for a particular attribute.
The first two use cases merge the IR and recommender scores
into a combined ranking.
The last two will let the recommender systems do the ranking
in situations where the IR model is not able to properly rank the results.


\input{table.rank.star}
\afterpage{\clearpage}

\subsection{Merging IR \& Recommender Scores}

\emph{(1) Multiple topics:} Let us start with a simple use case.
A user wishes to find movies about two separate topics, ranked by 
query match and predicted ratings.
This is a realistic use case, for example if a user is interested
in a few topics and wants to see the movie within these categories
he or she will probably like the most.
The IR algorithm takes care of finding the items within the categories,
while the adaptive recommenders finds the most enjoyable movies,
according to the metrics most preferred by this user in the past.

Table \ref{table:rank:washington} shows this use case and how our algorithm performs.
The results are for the query [\emph{``new york'' or washington}].
The first table shows the IR scores for the first 10 results,
and their rank (position in the list) according to these scores.
The second table shows the predicted rankings for each of these items.
Finally, the third table shows the ranking after the IR scores
and predicted ratings have been combined.
The final column shows how each item have moved in relation to the 
IR results list.

In this run of the algorithm, the IR weight ($w_{IR}$) was set to $1.0$,
instructing the algorithm place about the same importance on the IR score
and the predicted ratings. As we can see in the last section of 
Table \ref{table:rank:washington} the final result list is a blend
of the IR rankings and prediction rankings.
In other words, we have achieved personalized search. The results
from the IR method are re-ranked according to personal preferences.


\emph{(2) Series of items:} Let us consider another use case.
A user wishes to see a movie in a certain series of movies,
but does not know which one. In this case, the IR method can find all movies within this series,
while the recommender systems ranks the result list according to the user's preferences.

Table \ref{table:rank:startrek} shows the intermediary and final rankings
for the query [\emph{star trek}], which refers to a collection of movies within the same series.
The IR method returns all items that match this query,
and the recommenders predict the rating for each of these items.
However, since the IR method only ranks results based on how well they match the query,
and the recommenders only care about the predicted rating, the combined result
list can get the best of both worlds:
the top ranked items are the ones that both match the query \emph{and}
are probable good fits for the current user.


\subsection{Constraining the Item Space}

\emph{(3) Singular topic:} What happens when the IR weight is set to $0$?
In this use case, the predicted ratings alone sort the final list.
Consider the following use case.
A user wishes to see a movie related to a certain topic, e.g. a city.
Table \ref{table:rank:paris} shows two results lists for the query [\emph{paris}].
On the left are the standard ranking as returned by the IR model for this query,
along with their respective scores.
On the right we see the same results, re-ranked by user preferences.

For simple one-word queries, ignoring the IR score seems to give us the desired effect.
When we can be sure that each item returned by a search have the same textual relevance
(IR score), the IR method does not have any more information on which to rank
the results. The ranking then becomes the task of the recommender systems.
By employing adaptive recommenders, the results are not only ranked by 
one or more recommenders as chosen by the system, but by those of the recommenders
best suited to the current user. At the same time, each of these recommenders
are used differently for each item in the list, based on how well they have
performed for each item in the past.

\emph{(4) Attribute search:} 
As we can see, ignoring the IR score gives us quite a different algorithm.
Now, the search part is only performed to constrain the item-space worked
on by the recommender systems.
Another example of this is shown in Table \ref{table:rank:year}.
In this scenario, the user wishes to see a movie from a certain year,
and issues the query [\emph{1998}].
Naturally, the IR algorithm returns a whole lot of items, and each can
be said to be perfect answers to the query, as each movie was made in 1998.

In this case, setting the IR weight to $0$ allows us to rank the results
purely by predicted preference, which makes sense when the IR algorithm
can not rank the results in any meaningful way.
Note that the items in the left and right table are non-overlapping.
This is because only the first 10 results are shown.
The IR model returns a large number of items,
all with the same ranking score.
The recommender systems do the final ranking, and actually
push every item in the top 10 IR ranking 
below the top 10 final results.

\clearpage
\input{table.rank.paris}
\input{table.rank.year}
\clearpage

\subsection{Adaptive IR Weights}

As we have seen in this section, adaptive recommenders can provide personalized search
in multiple ways. 
By varying the IR weight we can create quite a range of systems:
On the one hand, an IR weight of 0 will let the recommenders do all the ranking.
On the other hand, by increasing the IR weight, the recommenders will carefully
adapt parts of the IR results list by moving some of the items.

We have not considered which IR weight or other parameters would result in the best performing
personalized search system.
However, this is completely dependent on the type of system and types of queries.
By varying the IR weight, a number of different systems that work for different use cases
can be constructed. For systems with simple one-word queries, setting the weight
to $0$ leaves ranking to the recommenders.
For systems with more complex queries, an IR weight of $1.0$ 
orders items both by IR score and predicted rating.
Naturally, this weight is a defining characteristic of any 
personalized search based on adaptive recommenders.

An adaptive adjustment of the IR weight based on the current query and use case
would seem to be the best choice for a system that should handle each scenario.
When we have a short, specific query, the IR ranking function have little to
no basis for ranking each item differently.
In this case, a weight of $0$ allows the recommenders to perform the ranking
of a constrained item space.
If the predicted ratings of each item is a constructive signal,
as in our movie search example, this will leverage the strenghts of both systems.

However, in systems with complex queries, where the IR model
\emph{can} rank each item based on their similarity to the information need,
an IR weight of $1.0$ gives us the desired result.
Items are ranked both by their match to the query,
and based on how each user and item should be modeled.
By using adaptive recommenders, each rescoring function
that influence the final rankings of items is conditioned
on how well they have previously worked for the current item,
and how well they suit each user.

The performance of personalized search is hard to judge without
extensive query logs with click-through information.
While we had no access to such data, 
we have been able to show that \emph{adaptive recommenders
can be used to provide personalized search}.
This positive result for Experiment 3 confirms hypothesis H3,
at least for this dataset, this IR system and our chosen recommender algorithms.
By confirming H3, we have shown that adaptive recommenders can be used for personalized search.

This results in a search engine results page that inherits the strenghts of adaptive recommenders.
Each item on the result list is ranked not just based on query matching,
but based on a number of signals, represented by recommender systems.
Each signal is adaptively used based on how well it suits the current user,
and how well it has worked in the past for each item.

This adaptive results page will hopefully help mitigate the latent subjectivity problem,
by ranking each element based on the current context.
We will discuss this further in the next chapter.


