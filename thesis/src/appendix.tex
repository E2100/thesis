\addtocontents{toc}{\protect\setcounter{tocdepth}{0}}

\chapter{Implementation}
\label{appendix:implementation}

This section describes how the system outlined above was implemented,
in order to perform the experiments of the next chapter.
This is a short description of the most important features and considerations
made when implementing the system.
While quite specific and not important to the viability of 
\emph{stacked user modeling} in itself,
this should give a short introduction to how this technique can be put into practice.

\section{Libraries}

Naturally, the most important part of the implementations are the recommender systems.
These are used for the basic ratings predictions, and to 
create the adaptive aggregation by predicting the accuracy of other recommenders.
At the same time, these different recommenders need to have the same
interface for training and testing, regardless of which context
each experiment places them into.

To quickly get a large number of recommenders up and running,
the system was linked with the \emph{Apache Mahout machine learning library}\footnote{
See \url{http://mahout.apache.org} --- accessed 09/05/2011}. 
Apache Mahout provides a number of machine learning
algorithms, amongst which a set of recommender systems.
Examples include SVD- and KNN-based recommenders,
baseline recommenders, a Slope One recommender,
cluster-based recommenders,
and various generic recommenders for mixing different 
similarity and neighborhood measures.
Mahout is a young project, launched in 2008, 
but was found to be quite mature and feature-rich
in our experience.

Mahout is build on top of \emph{Apache Hadoop},
a system for creating scalable and distributed data processing systems\footnote{
See \url{http://hadoop.apache.org/} --- accessed 09/05/2011}.
This is important to the performance of our system.
As mentioned, a lot of the operations performed in stacked user modeling
are independent and lend themselves well to parallelization.
By building on Hadoop, each of our recommenders come implemented in a 
proper MapReduce framework for parallel computation (as explained in \citet[p75]{Manning2008}).
Each of the basic recommenders and adaptive aggregators can then be modeled at the same time,
making the most out of whatever hardware is present.

For our IR tasks, we chose to build on another library.
\emph{Apache Lucene}\footnote{
See http://lucene.apache.org/ --- accessed 09/05/2011} is an open-source search engine, also built on top of Hadoop,
gaining the same performance wins as Mahout.
Lucene provides powerful methods for creating indexes of items, and for querying these indexes.

Mahout and Hadoop are libraries written in the Java Programming Language,
and runs on the Java Virtual Machine (JVM).
To facilitate rapid prototyping, the Ruby scripting language was chosen as a "glue" language,
for interfacing with the libraries. 
By using the JRuby\footnote{
See \url{http://www.jruby.org/} --- accessed 09/05/2011} 
implementation of Ruby, Java libraries can be imported directly
into the language, allowing us to use Mahout and Hadoop almost as if they were written in the same language.
The use of Ruby allowed us to quickly develop different combinations of recommenders and
perform varying experiments in a short amount of time.

\section{Task Structure}

In order to facilitate rapid prototyping,
our system is built around a few core concepts that can
be used together in different ways.
Everything the system does is considered a \emph{task}.
A task is a collection of settings and directives
and serves as instantiated configurations for the system.
Tasks are created beforehand, and fed into the sytem,
which carries them out.
Tasks specify what the system should do,
which dataset should be used, and other options.
See Appendix \ref{appendix:implementation} for an example.

The most important task is creating a recommender.
As recommenders are used both for the standard rating predictions,
and for the adaptive error estimations, creating recommenders
are the most common and important task of this system.
Another important task is creating evaluators.
An evaluator takes a set of recommenders as input,
tests them against the dataset specified in the task,
and returns the results of the evaluation.


\section{Modeling and Prediction}

The modeling phase consists of running our modeling algorithms and storing the resulting models.
A task is created for each of the basic recommenders, and for each of the adaptive recommenders.
If this is a rank aggregation scenario, an IR model is also created, based on the data
specified by the current task.
As mentioned, this is an offline approach, so that the models can be computed and recomputed,
indepentent of making any actual predictions.

As we shall soon explain, our experiments require us to measure the performance of each
recommender, and the stacked user modeling recommender, for every combination of 
a user and an item.
In order to perform these experiments, an \emph{evaluator} module was built.
As both the standard recommenders and the adaptive recommender system presents the same 
interface, the evaluators simply takes a set of recommenders as input, 
and measures their accuracy across the dataset specified by the current task.

As mentioned, none of these aspects have any bearing on the viability of stacked user modeling.
However, as this does provide an example of how to implement such a system. 
See Appendix \ref{appendix:resources} for links to other resources. 




\chapter{Expanded Results}

lorem


\chapter{Resources}
\label{appendix:resources}

