\section{Three Experiments}

Table \ref{table:experiments} shows the experiments that will test our technique.
Experiments 1 \emph{\&} 2 will test hypotheses H1 \emph{\&} H2.
We will measure the performance of adaptive recommenders compared to standard and aggregate recommenders.
Experiment 3 will test hypothesis H3.
We will try using our method to personalize sets of search results in a number of ways.
The first two experiments will be quantitative measurements of prediction aggregation performance.
The last experiment will be a qualitative exploration of personalized search with adaptive recommenders.
In particular, we will look at how different prioritizations of the IR model scores influence the final rankings.

\vspace{1em}
\begin{table}[h]
  \begin{tabular*}{\textwidth}{ l l l l l l l }
    \toprule
      ~Â & 
      \emph{mission} &
      \emph{hypotheses} &
      \emph{dataset} &
      \emph{users} &
      \emph{items} &
      \emph{ratings} \\
    \midrule
    
    Experiment 1 &
    pred.agg. &
    H1, H2 &
    MovieLens &
    943 &
    1,682 &
    100,000 \\
    
    Experiment 2 &
    pred.agg. &
    H1, H2 &
    Jester &
    24,983 &
    100 &
    1,832,275 \\

    Experiment 3 &
    rank.agg. &
    H3 &
    MovieLens &
    6,040 &
    3,900 &
    1,000,000 \\
    
    \bottomrule 
  \end{tabular*}
  \caption[List of Experiments]{List of Experiments performed in this chapter.}
  \label{table:experiments}
\end{table}

\noindent
As seen in Table \ref{table:experiments}, 
we will use two distinct datasets in the experiments.
Each dataset have different numbers of items, users and ratings.
This is a desirable property.
Testing adaptive recommenders in different scenarios
will help us achieve more reliable results.

First is the MovieLens dataset\footnote{
See http://www.grouplens.org/node/73 --- accessed 10.05.2011}.
This dataset is often used to test the performance of recommender systems
(as described in 
\citet[p.9]{Alshamri2008}, \citet[p.4]{Lemire2005}, \citet[p.1]{Adomavicius2005} and \citet[p.2]{Herlocker2004}).
It consists of a set of users, a set of movies, and a set of movie ratings from users,
on the scale $1$ through $5$.
We chose two subsets of the entire MovieLens collection.
For Experiment 1 we use a subset of 100,000 ratings from 943 users on 1,682 movies.
For Experiment 3 we use a much larger subset in order to
have more items available for the IR model.
This subset has 1,000,000 ratings from 6,040 users of 3,900 movies.

The MovieLens dataset also comes with meta-data on users, such as
gender, age and occupation. There is also meta-data on movies,
such as its title, release date and genre. 
In Experiment 1, we are only interested in the ratings matrix of this dataset.
The titles of movies will be used in Experiment 3.

Our second set of ratings comes from the Jester dataset\footnote{
See \url{eigentaste.berkeley.edu/dataset/} ---
accessed 22.05.2011}.
This is a set of 100 \emph{jokes} rated by users on a continuous scale.
As with MovieLens, this dataset is also commonly used
to test recommender systems (as described in
\cite{Goldberg2001}, \citet[p.14]{Herlocker2004}, \citet[p.5]{Adomavicius2005} and \citet[p.30]{Ahn2004}).
This dataset has many more users than those used in the other experiments.
On the other hand, there are significantly fewer items than in the other dataset.
The widely varying number of items, users and ratings in our selected datasets
will give us more dimensions along which to verify our results.

Jester also has ratings on a different scale than the MovieLens dataset.
While the movies are rated on a discrete scale from $1$ through $5$,
the items in Jester are rated on a continuous scale from $-10$ to $10$.
However, in order to easily compare the measurements on both datasets,
the ratings in Jester were converted to be on the scale $1-5$.
Still, the difference between ordinal and continuous ratings remains,
and will give us another differing quality to verify our results.

In another effort to achieve reliable results, 
both datasets were split into multiple disjoint subsets.
We need disjoint subsets in order to perform cross-validation testing.
This entails running the same experiments across all subsets and averaging the results.
Each dataset is split into five sets which are again split into training and testing sets:

\begin{eqsp}
  D_n = \{ d_1 = \{base_1, test_1\}, d_2 = \{base_2, test_2\}, ..., d_5 = \{base_5, test_5\} \}
\end{eqsp}

The $base_x$ and $test_x$ pairs are disjoint 80\% / 20\% splits of the data in the subsets.
We shall perform five-fold cross-validation across all these sets in our experiments.
This way we can be more certain that our results are reliable,
and not because of local effect in parts of the data.
As previously explained, the $base$ sets are further split using bootstrap aggregation,
into random subsets for training the standard recommender models.
The entire base set is then used to train the adaptive error estimating recommenders.
The corresponding $test$ sets will be used to evaluate our performance on the subsets.

Before performing our experiments,
let us take a closer look at the different types of recommender systems we will use.
