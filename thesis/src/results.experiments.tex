\section{Three Experiments}

Table \ref{table:experiments} shows the experiments that will test our technique.
Experiments 1 \emph{\&} 2 will test hypotheses H1 \emph{\&} H2.
We will measure the performance of stacked recommenders compared to standard and aggregate recommenders.
Experiment 3 will test hypothesis H3.
We will try using our method to personalize sets of search results in a number of ways.
The first two experiments will be quantitative measurements of prediction aggregation performance.
The last experiment will be a qualitative exploration of personalized search with stacked recommenders.
In particular, we will look at how different prioritizations of the IR model scores influence the final rankings.

\vspace{1em}
\begin{table}[h]
  \begin{tabular*}{\textwidth}{ l l l l l l l }
    \toprule
      ~Â & 
      \emph{mission} &
      \emph{hypotheses} &
      \emph{dataset} &
      \emph{users} &
      \emph{items} &
      \emph{ratings} \\
    \midrule
    
    Experiment 1 &
    pred.agg. &
    H1, H2 &
    MovieLens &
    943 &
    1,682 &
    100,000 \\
    
    Experiment 2 &
    pred.agg. &
    H1, H2 &
    Jester &
    24,983 &
    100 &
    1,832,275 \\

    Experiment 3 &
    rank.agg. &
    H3 &
    MovieLens &
    943 &
    1,682 &
    100,000 \\

    
    \bottomrule 
  \end{tabular*}
  \caption[List of Experiments]{List of Experiments performed in this chapter.}
  \label{table:experiments}
\end{table}

\noindent
As seen in Table \ref{table:experiments}, 
we will use two distinct datasets in the experiments.
Each dataset have different numbers of items, users and ratings.
This is a desirable property:
testing our technique in different scenarios
will help us achieve more reliable results.

First is the MovieLens dataset\footnote{
See http://www.grouplens.org/node/73 --- accessed 10.05.2011}.
This dataset is often used to test the performance of recommender systems
(as described in 
\citet[p9]{Alshamri2008}, \citet[p4]{Lemire2005}, \citet[p1]{Adomavicius2005} and \citet[p2]{Herlocker2004}).
It consits of a set of users, a set of movies, and a set of movie ratings from users,
on the scale $1$ through $5$.
We chose a subset of the entire MovieLens collection, with 100,000 ratings from 943 users on 1,682 movies.

The MovieLens dataset also comes with meta-data on each user, such as
gender, age and occupation. There is also meta-data on each movie,
such as its title, release date and genre. 
In Experiment 1, we are only interested in the ratings matrix of this dataset.
However, the titles of each movie will be used in Experiment 3.

Our second set of ratings comes from the Jester dataset\footnote{
See \url{eigentaste.berkeley.edu/dataset/} ---
accessed 22/05/2011}.
This is a set of 100 \emph{jokes} rated by users on a continous scale.
As with MovieLens, this dataset is also commonly used
to test recommender systems (as described in
\cite{Goldberg2001}, \citet[p14]{Herlocker2004}, \citet[p5]{Adomavicius2005} and \citet[p30]{Ahn2004}).
This dataset is considerably larger than our first set,
consisting of 1,832,275 ratings from 24,983 users of 100 items.
Notably, the number of items is quite smaller than in the other dataset.

Jester also have a different ratings scale compared to the MovieLens dataset:
while each movie is rated on a discrete scale from $1$ through $5$,
the items in Jester are rated on a continous scale from $-10$ to $10$.
However, in order to easily compare the measurements on both datasets,
the ratings in Jester were converted to be on the scale $1-5$.
Still, the difference between ordinal and continous ratings remains,
and will give us another differing quality to verify our results.

In another effort to achieve reliable evaluation results, 
both datasets were split into multiple disjoint subsets.
We need disjoint subsets in order to perform cross-validation.
This entails running the same experiments across all subsets and averaging the results.
Each dataset is split into five sets which are again split into training and testing sets:

\begin{eqsp}
  D_n = \{ d_1 = \{base_1, test_1\}, d_2 = \{base_2, test_2\}, ..., d_5 = \{base_5, test_5\} \}
\end{eqsp}
%
Each $base_x$ and $test_x$ are disjoint 80\% / 20\% splits of the data in each subset.
We shall perform five-fold cross-validation across all these sets in our experiments.
This way we can be more certain that our results are reliable,
and not because of local effect in parts of the data.
As previously explained, each $base$ set is further split using bootstrap aggregation,
into random subsets for training each stanard recommender model.
The entire base set is then used to train the adaptive error estimating recommenders.
Each corresponding $test$ set will be used to evaluate our performance on each subset.

Before diving into each experiment,
let us take a closer look at the different types of recommender systems each will use.
