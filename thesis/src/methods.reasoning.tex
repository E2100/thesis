\section{Latent Subjectivity}
\label{sec:reasoning}

As described in the previous chapter, 
there are many ways of predicting the
relevance of an item to a user. 
In fact, judging by the number of different approaches,
the only limiting factor seems to be the different 
patterns researchers discover in available data.

As described in Section \ref{sec:aggregate},
modern approaches to recommender systems try to combine many of these methods.
By leveraging so called \emph{disjoint patterns}
in the data, several less than optimal predictors
can be combined, so that the combination outperforms each part.
Moderns search engines work much in the same way,
combining multiple ranking signals into a final results list
(see Section \ref{subsec:signals}).

Why then, when we have all these valid approaches, would we need yet another technique?
As explained in Chapter \ref{chap:intro}, 
we believe the \emph{latent subjectivity problem}
is a fundamental issue with each approach described so far.
Consider the following examples of relevance judgement:

\begin{itemize*}
  \item PageRank \citep{Bender2005} assumes that the relevance of a web page is 
  represented by its authority, as computed from inbound links from other sites.
  \item Some systems considers a user's social connections to be important
  in ranking search results, when performing personalized search (e.g. \cite{Carmel2009}).
  \item When recommending movies, one predictor may be based on the ratings
  of users with similar profile details. Another predictor might be 
  dependent on some feature, e.g. production year of well liked movies.
  \item SVD-based approaches assume global patterns and groups of users and items
  are best suited to produce predictions (see Section \ref{subsec:recommender:examples}).
  \item Recommendations based on the Pearson Correlation Coefficient \cite[p11]{Segaran2007}
  assumes that the statistical correlation between user ratings is a good
  measure for user similarity.
\end{itemize*}

While these methods may perform well, their selection
reflects how whoever created the system assumes how each user
can and \emph{should} be modeled. This underlying subjectivity is not desirable.
We see two different approaches one can take when creating a recommender system,
represented by two questions:

\clearpage

\begin{enumerate*}
  \item What combination of which methods will accurately predict unknown scores?
  \item Which methods could possibly help predict a score for a user or an item?
\end{enumerate*}

The first question is what has to be considered in traditional modeling aggregation.
First, a set of applicable methods leveraging disjoint patterns must be selected. 
Then, an optimal and generalized combination of these must be found,
most often through minimizing the average error across all users.

The second question is quite different. 
Instead of looking for an optimal set of methods and an optimal combination,
we look for the set of \emph{any applicable methods} that \emph{some users} might find helpful,
or that might work for \emph{some items}.
We believe this is a much simpler problem. 
Instead of trying to generalize individuality,
it can be embraced, by allowing users to implicitly and automatically select which methods they prefer,
from a large set of possible predictors.

As explained in Chapter \ref{chap:intro}, 
assuming that each user will place a similar importance on each modeling method is 
contrary to the goals of user modeling. 
Our goal should instead be a system which can leverage the differences
between users and employ each available algorithm based on their individual preferences.

Just as each user is different, each item may have their own characteristics.
Needless to say, items are often quite different from another,
along a myriad of dimensions. For example,
if each webpage is an item, the number of metrics we can use to judge
the relevance of an item is immense.
If items are indeed as different as the users themselves, it stands to reason that the same 
modeling method will not perform as well for every item.

An approach where we only need to consider the second question is desirable.
Regardless, both traditional, single-approach modeling methods, and modern aggregation approaches
often treat every user and item the same. No matter its intrinsic qualities, an element will be judged
by the same methods as every other element. 

This chapter will develop a way to aggregate a host of modeling methods on a per-user and per-item basis.
By adapting the aggregation to the current item and user, we should be able to mitigate the latent subjectivity problem. 
This adaptive aggregation is performed implicitly and automatically,
without any extra interaction required from each user.

As mentioned in Chapter \ref{chap:intro}, this has an important consequence.
If each algorithm is \emph{only used} in situations it will probably perform well,
any possible recommender algorithm becomes a worthy addition.
In other words, instead of finding an optimal combination,
we can use any algorithm that \emph{might work for some elements}
in our aggregation.

With adaptive recommenders, each user is in control of which methods best fit their needs, and
each method's priority is influenced by how well it performs for the current item.
However, in order to test whether or not these assumptions hold,
we need a set of testable hypotheses.

\clearpage
