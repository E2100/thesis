\section{Aggregate Modeling}
\label{sec:aggregate}

So far we have seen a lot of modeling methods, both for recommender systems (RS) and for personalized search (PS).
\emph{Aggregate modeling} is the act of merging two or more modeling methods in some way.
A proper aggregation method creates a combined result that is better than either of the individual methods,
where the sum is greater than the parts.
We have already seen a few examples of aggregate modeling:

\begin{itemize*}
  \item \cite{Koren2008} aggregates global, individual and per-item averages to a baseline.
  \item \cite{Huang2002} aggregates different types of graph relations into one prediction.
  \item \citet{Haveliwala2003} combined their personalized PageRank with another approach.
  \item \cite{Carmel2009} combined classic IR with social relations and annotations.
  \item \citet[p.5]{Bender2005} aggregates signals measured from website structure.
\end{itemize*}

The reason for combining different approaches is that no one method can capture all the predictive nature of available data.
For example, 
\cite{Bell2007b} created a recommender system where the neighborhood- and SVD-based approaches complement each other.
While the neighborhoods correspond to \emph{local effects} where similar users influence each other's predictions,
the dimensionality reduction finds \emph{regional effects}, major structural patterns in the data \citep{Bell2007b}:

\begin{blockquote}
``Both the local and the regional approaches, and in particular their combination through a unifying model, 
compare favorably with other approaches and deliver substantially better results than the 
commercial Netflix Cinematch recommender system [...].''
\end{blockquote}

An interesting question is whether or not all hybrid recommenders,
that combine content-based and collaborative methods, are aggregators. 
This is mostly a question of semantics and implementation.
\citet[p.4]{Burke2007} liberally defines a hybrid system as 
\emph{``any recommender system that combines multiple recommendation techniques together to produce its output''}.
Some hybrid methods combine stand-alone methods, and are definitely aggregations.
Other methods merge the methods themselves into one implementation that uses the data in different ways.
\citeauthor{Burke2007} describes a few types of hybrid recommenders:

\begin{itemize*}
  \item Weighted combinations of recommenders.
  \item Switching and choosing one recommender in different contexts.
  \item Mixing the outputs and presenting the result to each user.
  \item Cascading, or prioritized recommenders applied in succession.
  \item Augmentation, where one recommender produces input to the next.
\end{itemize*}

However, without being to pedantic, these can all be seen as aggregate approaches. Multiple prediction techniques are used
to create a result better than any single methods would provide.
There are two main use cases for model aggregation \cite{Liu2007}: 

\begin{enumerate}
  \item Rank (or \emph{order-based}) aggregation (RA) lets a set of methods
  produce a sorted list of recommendations or search results. These lists are then combined
  into one final list, through some aggregation method (see \cite{Dwork2001} or \cite{Klementiev2008}).
  These methods only require the resulting list of items from each method \cite{Aslam2001}.

  \item Prediction (or \emph{score-based}) aggregation (PA) works on the item- or user-level by combining predicted scores
  one-by-one, creating an aggregated result for each element that should be evaluated.
  These methods require the actual prediction scores for any item from the recommender methods \cite[p.2]{Aslam2001}.
\end{enumerate}

\subsection{Rank Aggregation}
\label{sec:theory:rank}

RA combines multiple result lists into one list through aggregation.
\cite{Dwork2001} shows a few metrics applicable to meta-search, the combination of results from multiple search engines.
For example, Borda's method \cite[p.6]{Dwork2001} is based on positional voting, where each result gets a certain number of points from each result set,
based on where it appears in the sorted list. Items at the top gets the most points, while lower items gets fewer points.
This is in essence an approach where the predictors have a set number of votes ($c$, the number of results) that they give to the items.

As we saw in Section \ref{sec:search}, \citet[p.3]{Xu2008} used a weighted version
of this approach to combine an IR and personal approach to result ranking.
\citet[p.3]{Aslam2001} calls their version of this \emph{Weighted Borda-Fuse},
where the points given from a method to an item is controlled
by the weights estimated for the methods.
\citet[p.4]{Aslam2001} also explain a Bayesian approach (\emph{bayes-fuse}),
that combined with the \emph{naive Bayes} independence assumption 
produce the following formula:

\begin{eqsp}
  \mathrm{relevance}(d)  = \sum_{i \in Methods} \log 
    \frac{ \mathrm{Pr}(r_i(d) | rel) }{ \mathrm{Pr}(r_i(d) | irr) }.
\end{eqsp}

Here, $\mathrm{Pr}(r_i(d) | rel)$ is the probability that document $d$
is relevant given its ranking by method $i$.
Conversely, $\mathrm{Pr}(r_i(d) | irr)$ is the probability that the document 
is irrelevant. The probability values are obtained through training,
and evaluating the results against known relevance judgements.
An interesting note is that the standard Borda method does not require training data,
while the weighted version and the Bayesian approach do.
\citet{Aslam2001} achieved positive results with these methods:

\begin{blockquote}
``Our experimental results show that meta-search algorithms based on the 
Borda and Bayesian models usually outperform the best input system 
and are competitive with, and often outperform, 
existing meta-search strategies.''
\end{blockquote}

\cite{Liu2007} presents a rank-aggregation framework, where
the task of estimating a ranking function by using training data.
They treat this task as a general optimization problem, with results
showing that this framework can outperform existing methods \cite[p.7]{Liu2007}.

Rank aggregation is a substantial topic, with many approaches.
The main take-away is that this approach combines list of results
into one single results, and experiments show that results superior
to the best of the combined methods are attainable.
See \cite{Aslam2001}, \cite{Liu2007} or \cite{Klementiev2008} 
for more information.

%\clearpage

\subsection{Prediction Aggregation}
\label{sec:theory:predictionagg}

Unlike rank aggregation, prediction aggregation (PA) does not deal with lists of results.
PA works on the item-level, collecting scalar predictions of an item's relevance from a number of methods,
and combining these predictions into a final score.
This thesis is most concerned with prediction aggregation,
and it is where we shall evaluate the performance of our approach.

\cite{Aslam2001} describe a number of simple approaches.
For example, minimum, maximum and sum aggregations combine the individual 
predictions based on simple arithmetics, 
or select one or more of the results as the final prediction. 
Other models use the average, or log-average of the different methods.
Another example is the linear combination model, that trains weights for each predictor and weighs predictions accordingly.
At slightly more complex approach is to train a logistic regression model \cite[p.3]{Aslam2001}
over a training set, in an effort to find the combination that gives the lowest possible error.
This last method improved on the top-scoring predictor by almost 11\%,
showing that even fairly simple combinations have merit.

Early approaches to recommender systems experimented with aggregating content-based and collaborative approaches.
\cite{Claypool1999} combined the two approaches in an effort to thwart problems with each method.
Collaborative filtering (CF) methods have problems rating items for new users, radically different users or when dealing with very sparse data.
Content-based (CB) methods do not have the same problems, but are less effective than CF in the long run, as CB does not tap into the 
knowledge of other users in the system --- knowledge that out-performs simple content analysis.
In \cite{Claypool1999}, the two types of recommenders were used to create a simple weighted result.

Generally, methods for aggregating predictions in the field of machine learning is called \emph{ensemble methods} (EM) \cite{Dietterich2000}.
While most often used to combine classifiers that classify items with discrete labels,
these methods are also used for aggregating numerical values (see the numerical parts of \cite{Breiman1996}).
Approaches include \emph{bootstrap aggregation} (bagging) and \emph{boosting} 
for selecting proper training and testing sets,
and creating a \emph{mixture of experts} for combining the predictors
\cite[p.27]{Polikar2006}.

%\url{http://www.igvita.com/2009/09/01/collaborative-filtering-with-ensembles/}
%\url{http://en.wikipedia.org/wiki/Machine\_learning\_ensemble}
%\url{http://www.tdan.com/view-articles/4960/}

\citeauthor{Bell2007} took method aggregation to its logical conclusion when winning the Netflix Challenge,
by combining 107 individual results from different recommenders.
They found this to significantly outperform each standard recommender \cite[p.6]{Bell2007}:

\begin{blockquote}
``We strongly believe that the success of an ensemble approach depends on the ability of its various predictors to expose different, 
complementing aspects of the data. Experience shows that this is very different from optimizing the accuracy of each individual predictor. 
Quite frequently we have found that the more accurate predictors are less useful within the full blend.''
\end{blockquote}

Both rank and prediction aggregation are extensive topics.
In both cases, the take-away stays the same.
By combining different modeling methods,
more patterns in the data can be mined, and 
the resulting combination can outperform the best performing method.
This is key to the model we shall build in the next chapter.

