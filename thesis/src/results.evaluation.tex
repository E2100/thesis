\section{Evaluation Strategies}

To evaluate how our model performs during prediction aggregation, 
we need a measure for computing the total error across a large number of predictions.
The canonical measure for estimating the error of a recommender system
is the \emph{Root mean squared error} (RMSE) measure
(for example in \citet[p17]{Herlocker2004}, \citet[p13]{Adomavicius2005} and \citet[p6]{Bell2007}).
We shall use this measure to estimate the performance
of our adaptive prediction aggregation algorithms.
The RMSE of a set of estimations $\hat{R}$, 
compared the correct rating values $R$, is defined as

\begin{eqsp}
  \mathrm{RMSE}(\hat{R},R) = \sqrt{\mathrm{E}((\hat{R} - R)^2)}
  = \sqrt{\frac{
      \sum_{i=1}^{n} (\hat{R}_i - R_i)^2
    }{
      n
    }},
\end{eqsp}
%
where $n$ is the total number of predictions.
The RMSE combines a set of errors into one single combined error.
A beneficial feature of the RMSE is that the resulting error 
will be on the same scale as the estimations. For example,
if we are predicting values on the scale $1-5$, the computed error
will be on this scale as well. In this case, an error of $1$
would then say that we are on average $1$ point away from the true 
ratings on our $1-5$ scale.

RMSE is a non-linear error estimator.
This means that larger errors are harshly punished.
Because the differences are squared by the formula,
many small errors are much less significant than a few big errors.
In other words, the RMSE will judge methods that provide
stable predictions more favorably
than methods that, while precise, have a few items
or users for which the method breaks down.
For example, when the RMSE was used in the Netflix movie recommender challenge
\citep{Linden2009}, the participating teams
found that a few hard to predict movies often 
single-handedly severely impacted their total error.

While the RMSE works well for evaluating scalar predictions,
we need another measure for evaluating rank aggregation methods.
Here, we are not interested in the predicted scores,
but rather in which position each item appears in a sorted list of results.
This is for instance needed when measuring the performance of a
personalized search engine.
However, H3 does not state anything regarding explicit performance,
only that our method should be applicable in an information retrieval scenario.
The performance of personalized search is hard to determine,
as there are many types of rankings that make sense in a number of different use cases,
as we shall soon see.
Because of this, regarding hypothesis H3, 
we are most interested in examining how  personalization with adaptive recommenders 
affects the initial rankings from an IR method.



