\subsection{Glossary}

\begin{itemize*}
  \item IR: Information Retrieval 
  \item RS: Recommender System
  \item PS: Personalized Search
  \item D:  Set of Documents
  \item Q:  Set of Queries  
  \item C:  Set of Users
  \item U:  Set of Utilities/Ratings
  \item F:  Set of Relations
  \item M:  A Model of Computation
  \item u,v: User indices
  \item i,j: Document indices
  \item SERP: Search Engine Results Page
\end{itemize*}


\subsection{Information Overload}
\subsection{Information Retrieval}

"Information retrieval (IR) is finding material (usually documents)
of an unstructured nature (usually text) that satisfies an
information need, from within large collections 
(usually stored on computers)" - \cite[p1]{Manning2008}.

\begin{itemize*}
  \item The history of search, from libraries, to the web to massive datasets
  \item IR is all about finding documents
  \item Deals with information overload \cite{Bjorkoy2010d}
  \item User knows what he wants, but not where it is or if it exists
  \item Problem: Lack of data, increasing amounts of unreliable info, no personalization
  \item Problems: Polysemy, synonymy \cite[p4]{Micarelli2007}.
\end{itemize*}

\cite[p6]{Bender2005}: The Free Web:
"Another big difference between the web and traditional well controlled collections is that there is virtually no control over what people can put on the web. Couple this flexibility to publish anything with the enormous influence of search engines to route traffic and companies which deliberately manipulating search engines for profit become a serious problem. This problem that has not been addressed in traditional closed information retrieval systems."

\cite[p1]{Silverstein1999}: Web search differ from traditional IR:
"Our data supports the conjecture that web users differ significantly from the user assumed in the standard information retrieval literature. 
Specifically, we show that web users type in short queries, mostly look at the first 10 results only, and seldom modify the query. 
This suggests that traditional informa- tion retrieval techniques may not work well for answering web search requests. 
The correlation analysis showed that the most highly correlated items are constituents of phrases. 
This result indicates it may be useful for search engines to consider search terms as parts of phrases even if the user did not explicitly specify them as such."

\cite[p1]{Lee2005}: Navigational vs Informational queries: "The goal of a user can be classified into at least two categories: navigational and informational.
A query is considered navigational when a user has a particular Web page in mind and is primarily in- terested in visiting the page. 
Informational queries, on the other hand, refer to the queries where the user does not have a particular page in mind 
or intends to visit multiple pages to learn about a topic."

\cite[p1]{Beitzel2004}: Queries varies houerly:
"The queries received during peak hours are more similar to each other than their non-peak hour counterparts."
"Some topical categories vary substantially more in popularity than others as we move through an average day."

\cite[p2]{Micarelli2007}: Three information access paradigms: browsing, searching by query, recommendations.


\subsection{User Modeling}

\begin{itemize*}
  \item History of recommendations, from libraries, to the web
  \item UM about tailoring application to its users
  \item RS is all about recommending new items
  \item Also deals with information overload
  \item Recommendations in two categories: recommended and similar items
  \item Unlike in search, the user does not know what he wants
  \item Problem: One method is not enough for complex recommendations. People are different.
  \item Problems: Privacy \cite[p3]{Micarelli2007}.
\end{itemize*}



\subsection{Personalized Search}

IR and RS/UM, two sides of same coin.
RS is IR with an active item as basis for similiarity.
IR is RS with the query as the active item as basis for recommendations/similarity.
Combining should be a trivial, common task.

\cite[p1]{Chirita2010}: Query expansion:
"Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly. "

\begin{itemize*}
  \item Social search often a different meaning -- search social networks
  \item Why they complement each other
\end{itemize*}



\subsection{Composite User Modeling}

\begin{itemize*}
  \item More data beats better algorithms -- users are more data, 
        a fact often neglected by traditional IR
  \item Combining many algorithms beats one single algorithm, e.g. Netflix.
  \item More algs equals more data!
\end{itemize*}

\cite[p12]{Bender2005}: Aggregate search hit rankings. Already from earliest Google:
"The ranking function has many parameters like the type-weights and the type-prox-weights. Figuring out the right values for these parameters is something of a black art. In order to do this, we have a user feedback mechanism in the search engine. A trusted user may optionally evaluate all of the results that are returned."


\subsection{Hypothesis \& Contributions}

\begin{itemize*}
  \item $H_0$: A User Model can be created through a combination of different personalization algorithms to provide personalized IR.
  \item $H_2$: The personalized search performed by such models outperforms single algorithms and pure IR in the Rank Scoring and Average Rank metrics.
\end{itemize*}

Contributions:

\begin{itemize*}
  \item Framework for providing IR with UM/personalization
  \item More data for IR methods to leverage
  \item System for creating aggregate user models
\end{itemize*}



\subsection{Outline}
