\label{chap:intro}

Information is a curious thing:
having too much can be as harmful as having no information at all.
While lacking information is an obvious problem,
too much leads to information overload,
where relevant content drowns in irrelevant noise.
This is a common problem: whenever we have enough information,
any extra irrelevant data only leads to confusion.
Our ability to make informed decisions is often the first thing to go
\cite[p1]{Davenport2001}.

However, while people struggle with excessive information,
algorithms in artificial intelligence can not get enough.
\citet[p1]{Halevy2009} calls this the \emph{unreasonable effectiveness of data}:
perhaps surprisingly, more data often trumps more efficient algorithms.
\citet[p3]{Banko2001} shows how common algorithms in AI can improve substantially by giving them a lot more data to work with.
As much as researchers chase elegant algorithms, finding more data to work with may be time better spent.

Few places is this difference of users and computers more apparent than in \emph{recommender systems}.
A recommender system is a technique in user modeling to estimate the relevance of an item to a user.
An item can be just about anything: documents, websites, movies, events or other users.
Recommender systems are especially prolific on the Web: 
wherever there is personalized recommendations of news, books, movies,
articles, social connections, search results, et cetera, recommender systems are working behind the scenes.

Modern recommendation approaches often embrace this unreasonable effectiveness of data,
by combining multiple recommender systems, that each predict relevance in various ways.
By considering different aspects of users and items when making predictions,
the methods provide quite complex predictions that rely on much evidence.
These systems use data such as search query logs, relevance judgements from similar users,
social connections and much more, as we shall soon see.
For example, \citet[p1]{Bell2007} took this to its logical conclusion by 
combining 107 different recommender systems when winning the Netflix movie recommendation competition
(see \citet{Linden2009}).

While the name might seem constraining, recommender systems are incredibly powerful tools.
If we can accurately predict how each user will react to each item,
we will have come a long way towards solving information overload.
Likewise, chronological sorting of items would be a figment of a simplistic past,
as sorting based on actual relevance would be far superior.

However, despite their apparent power, in the real world, recommender systems are often confined
to menial tasks like creating small lists of recommended items, 
or meekly suggesting similar items to the ones being considered by a user.
Common examples are lists of recommended items based on the one being viewed, 
recommending new social connections, or suggesting news articles based on previous reading.
Seldom are their full potential reached by creating completely adaptive
content systems, that work hard to mitigate any signs of information overload.

We posit that traditional recommender systems have an important weakness.
There exists an underlying, misplaced subjectivity to relevance prediction.
We believe this fundamental weakness hinders the full adoption of these systems.
There is a mismatch between how recommender systems perform predictions,
and how each user and item wants this prediction to be made.

Consider this: 
when an algorithm is developed or selected for use in a recommender system,
there is a concious descision of which predictive data pattern to use.
Before any user modeling is performed, the researcher or developer selects
one or more methods that is thought to best model every user and item in the system.
While the methods themselves may perform well, their selection
reflects how whoever created the system assumes how each user
can and \emph{should} be modeled. This underlying subjectivity is not desirable.
We call this the \emph{latent subjectivity problem}.

Examples are not hard to come by.
For instance, while one user might appreciate social
influence in their search results, another user might not.
While one user might find frequency of communication maps well to relevance,
another might not. 
One user might feel the similarity of movie titles are a good predictor,
while another might be more influenced by production year.
Some users may favor items rated highly on a global scale,
while others are more interested in what users similar to themselves have to say.
The same problem exists for items: while one item might best be judged by its content,
another might be better described by previous ratings from other users.
One item's relevance may be closely tied to when it was created,
while other items may be timeless.
The exact differences are not important --- what is important is that they exist.

Another way of explaining the latent subjectivity problem is that 
\emph{user modeling methods are dependent on the subjective assumptions of their creators}.
In other words, a modeling method use some aspect of available data to make predictions,
and this aspect is chosen by whoever creates the system.

Aggregate modeling methods face the same problem of misplaced subjectivity: 
Aggregation is done on a generalized, global level,
where each user and item is expected to place the same importance on each modeling method.
While the aggregation is of course selected to minimize some error over a testing set,
the subjective nature remains: the compiled aggregation is a generalization,
treating all users the same --- hardly a goal of user modeling.

Should it not be up to each user to implicitly decide which method best describes their preferences?
And, considering the vast scope of items we can come by, will the selected
methods really perform optimally for every item?
We believe the priority of each algorithm should be implicitly and automatically
based on how well they have previously worked for the current user and item.
Without this adaptability, it may be hard for recommender systems
to gain traction in scenarios with widely differing users and items.
The scope of users and items is simply too great for any one or generalized combination
of methods to capture the nuanced nature of relevance prediction.

We propose a novel method called \emph{stacked recommenders}, where these descisions are left to each user and item,
providing an extra level of abstraction and personalization.
The descisions are implict, and happens in the background, without any extra interaction required.
This leaves the subjective nature of selecting ways to model users and items where it should be:
in the hands of each individual user, and dependent on each specific item, without any extraneous effort.
If each method of relevance prediction is \emph{only used} based on how well it performs for each element,
any possibly applicable recommender system suddenly becomes a worthy addition to the system.

As far as we know, this kind of adaptive prediction aggregation has not been done before.
Can stacked recommenders help standard recommender systems reach their full potential?
Can a system where each user and item implicitly decides how they should be modeled outperform traditional approaches?
\emph{That is the main research question of this paper}.

%\hr

This paper is structured as follows.
Chapter \ref{chap:theory} will present background theory and previous work for
the information overload problem, user modeling, recommender systems and personalized search. 
Chapter \ref{chap:methods} will build the \emph{stacked recommenders} approach from the ground up.
Chapter \ref{chap:results} will test three hypotheses and experiment with our newly built model.
We will try prediction aggregating for singular items, and rank aggregation for personalized search.
Finally, Chapter \ref{chap:discussion} will discuss the implications of our results,
important limitations, contributions and suggest future work.


