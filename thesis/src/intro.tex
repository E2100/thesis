\label{chap:intro}

Information is a curious thing:
having too much can be as harmful as having no information at all.
While lacking information is an obvious problem,
too much leads to information overload,
where relevant content drowns in irrelevant noise.
As the amount of information grows,
our ability to make informed descisions quickly diminish.
Our amount of available attention is limited, and each new bit
of information consumes it further
\cite[p1]{Davenport2001}.

However, while people struggle with excessive information,
algorithms in artificial intelligence simply can not get enough.
\citet[p1]{Halevy2009} calls this the \emph{unreasonable effectiveness of data}:
perhaps surprisingly, more data often trumps more efficient algorithms.
\citet[p3]{Banko2001} shows how traditional algorithms can be substantially
improved by giving them a lot more data to work with.
As much as researchers chase elegant algorithms,
finding more data to work with may be time better spent.

Few places is this difference of users and computers more apparent than in \emph{recommender systems}.
A recommender system is a technique in user modeling to estimate the relevance of an item to a user.
An item can be anything: documents, websites, movies, events, or indeed, other users.
These techniques are especially prolific on the Web: 
wherever there is personalized recommendations of news, books, movies,
articles, social connections, search results, et cetera, recommender systems are the tools
working behind the scenes.

Modern recommender systems often embrace the unreasonable effectiveness of data,
by combining multiple recommender methods that predict relevance in various ways.
By considering different aspects of users and items when making predictions,
the methods provide quite complex predictions that rely on much evidence.
These systems can use data such as search query logs, relevance judgements from similar users,
social connections and much more, as we shall soon see.
For example, \citet[p1]{Bell2007} took this to its logical conclusion by 
combining an astonishing 107 different recommenders when winning the Netflix recommendation competition
(see \citet{Linden2009}).

While the name might seem constraining, recommender systems are incredibly powerful tools.
If we can accurately predict how each user will react to each item, the information overload problem would be solved.
Likewise, chronological sorting of items would be a figment of a simplistic past.
User interfaces could provide optimal selection and placement of information to each user.

However, despite their apparent power, in the real world, recommender systems are often confined
to menial tasks like creating small lists of recommended items, 
or meekly suggesting similar items to the ones being considered by a user.
The primary examples are recommended movies, social connections or news articles.
Seldom are their potential reached by creating completely adaptive
content systems, that work hard to mitigate any signs of information overload.

We posit that traditional recommender systems have an important weakness.
There exists an underlying, misplaced subjectivity to relevance prediction.
This fundamental weakness hinders the full adoption of these systems.

Consider this: 
when a method is developed or selected for use in a recommender system,
a concious descision of which approach to use is made.
Before any user modeling is performed, the researcher or developer selects
one or more methods that is thought to best model every user and item in the system.
While the methods themselves may perform well, their selection
reflects how whoever created the system assumes how each user
can and \emph{should} be modeled. This underlying subjectivity is not desirable.
We call this the \emph{latent subjectivity problem}.

Examples are not hard to come by.
For instance, while one user might appreciate social
influence in their search results, another user might not.
While one user might find frequency of communication maps well to relevance,
another might not. 
One user might feel the similarity of movie titles are a good predictor,
while another might be more influenced by production year.
The exact differences are not important --- what is important is that they exist.

The same goes for items: while one item might best be judged by its content,
another might be better described by previous ratings from other users.
One item's relevance may be closely tied to when it was created,
while other items may be timeless.

Should it not be up to each user to implicitly decide which method best describes their preferences?
And, considering the vast scope of items we can come by, will the selected
methods perform similarly for every item?
How much each algorithm is used should be used implicitly and automatically
based on how well they have previously worked for the current user.
At the same time, each algorithm should only be seriously considered
when considering items is has worked well for in the past.
Without this adaptability, it may be hard for recommender systems
to gain traction in scenarios with widely differing users and items.

Another way of explaining the latent subjectivity problem is that 
\emph{user modeling methods are dependent on the subjective assumptions of their creators}.
In other words, a modeling method use some aspect of available data to make predictions,
and this aspect is chosen by whoever creates the system.

Aggregate modeling methods face the same problem of misplaced subjectivity: 
Aggregation is done on a generalized, global level,
where each user and item is expected to place the same importance on each modeling method.
While the aggregation is of course selected to minimize some error over a testing set,
the subjective nature remains: the compiled aggregation is a generalization,
treating all users the same --- hardly a goal of user modeling.

We propose a novel method called \emph{stacked recommenders}, where these descisions are left to each user and item,
providing an extra level of abstraction and personalization.
The descisions are implict, and happens in the background, without any extra interaction required.
This leaves the subjective nature of selecting ways to model users and items where it should be:
in the hands of each individual user, and dependent on each specific item, without any extraneous effort.
If each method of relevance prediction is \emph{only used} based on how well it performs for each element,
any possibly applicable recommender system suddenly becomes a worthy addition to the system.

As far as we know, this kind of adaptive prediction aggregation has not been done before.
If we can construct an ensemble system that leverage each algorithm
based on how well it will perform for the current user and item,
we will have come closer to solving the latent subjectivity problem.

Can stacked recommenders help standard recommender systems reach their full potential?
Can a system where each user and item implicitly decides how they should be modeled outperform traditional approaches?
\emph{That is the main research question of this paper}.

%\hr

This paper is structured as follows.
Chapter \ref{chap:theory} will present the background theory needed to develop our modeling method:
the information overload problem, user modeling and recommender systems and personalized search. 
Chapter \ref{chap:methods} will build the \emph{stacked recommenders} approach from the ground up.
Our approach will allow us to adaptively combine different recommenders
on a per-user and per-item level, sidestepping the the latent subjectivity problem.
Chapter \ref{chap:results} will test three hypotheses and experiment with our newly built model.
We will try prediction aggregating for singular items, and rank aggregation for personalized search.
Finally, Chapter \ref{chap:discussion} will discuss the implications of our results,
their limitations, our contributions and suggest future work.


